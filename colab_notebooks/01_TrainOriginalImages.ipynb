{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-Train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KKE6KGPy-I7z",
        "dmhnfzo8p2wf",
        "N7gO9YJxwIjV",
        "mlJsxzBKw9vl"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N1-zk0B8JEK"
      },
      "source": [
        "# Train\r\n",
        "\r\n",
        "Colab to test the pix2pix training based on the code found in https://github.com/mrzhu-cool/pix2pix-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EyyleZS8ZLh"
      },
      "source": [
        "## Imports and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnj7r6_2_Hmi"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSkR6PpC8czK"
      },
      "source": [
        "# Accessing the files and preparing the dataset\r\n",
        "from google.colab import drive\r\n",
        "from os import listdir\r\n",
        "from os.path import join\r\n",
        "import os\r\n",
        "\r\n",
        "# Treating the images\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import torch\r\n",
        "import torch.utils.data as data\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from matplotlib.pyplot import imshow\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Dealing with GPUs\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "\r\n",
        "# Defining the networks\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import init\r\n",
        "import functools\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "# Training\r\n",
        "from math import log10\r\n",
        "import time\r\n",
        "import math\r\n",
        "\r\n",
        "# Tensorboard\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBzHI4aC_Jxo"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDGdBS9o8iOC"
      },
      "source": [
        "import argparse\r\n",
        "\r\n",
        "# Training settings\r\n",
        "parser = argparse.ArgumentParser(description='pix2pix-pytorch-implementation')\r\n",
        "# In the original code, dataset is required. We don't need it for the Inria Aerial Image Labelling Dataset\r\n",
        "parser.add_argument('--dataset', required=False, help='facades')\r\n",
        "parser.add_argument('--batch_size', type=int, default=1, help='training batch size')\r\n",
        "parser.add_argument('--test_batch_size', type=int, default=1, help='testing batch size')\r\n",
        "parser.add_argument('--direction', type=str, default='a2b', help='a2b or b2a')\r\n",
        "parser.add_argument('--input_nc', type=int, default=3, help='input image channels')\r\n",
        "parser.add_argument('--output_nc', type=int, default=3, help='output image channels')\r\n",
        "parser.add_argument('--ngf', type=int, default=64, help='generator filters in first conv layer')\r\n",
        "parser.add_argument('--ndf', type=int, default=64, help='discriminator filters in first conv layer')\r\n",
        "# Training epochs are defined by range(opt.epoch_count, opt.niter + opt.niter_decay + 1)\r\n",
        "# So, originally, the training script epochs from 1 to 201, which takes too long at the beginning\r\n",
        "# niter and niter_decay are changed to shorten the amount of time during development\r\n",
        "parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count')\r\n",
        "parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate') # 100\r\n",
        "parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero') # 100\r\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam') # 0.0002\r\n",
        "parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau|cosine')\r\n",
        "parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\r\n",
        "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\r\n",
        "parser.add_argument('--cuda', action='store_true', help='use cuda?')\r\n",
        "parser.add_argument('--threads', type=int, default=1, help='number of threads for data loader to use')\r\n",
        "parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\r\n",
        "parser.add_argument('--lamb', type=int, default=10, help='weight on L1 term in objective') # 10\r\n",
        "# Activate or deactivate the use of Tensorboard\r\n",
        "parser.add_argument('--tb_active', type=bool, default=True, help='should tensorboard be used') # Deactivate for deep trainings\r\n",
        "# Which original image should be stored in Tensorboard.\r\n",
        "# Inria satellite images are 5000x5000 and consume much CPU and memory, so only\r\n",
        "# one image is saved to avoid using too many resources\r\n",
        "parser.add_argument('--tb_image', type=str, default='vienna1.tif', help='image to store in tensorboard')\r\n",
        "# Number of images saved to tensorboard. Only tb_image will be saved, so the progress\r\n",
        "# of generated images can be seen throw epochs. 5 images in 100 epochs means one\r\n",
        "# tb_image will be saved every 20 epochs.\r\n",
        "parser.add_argument('--tb_number_img', type=int, default=5, help='number of images saved to tensorboard')\r\n",
        "# Level of debug (cell output)\r\n",
        "parser.add_argument('--debug', type=int, default=0, help='level of debug from 0 (no debug) to 2 (verbose)')\r\n",
        "# Number of iteration messages per epoch. They have the form\r\n",
        "# ===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\r\n",
        "parser.add_argument('--iter_messages', type=int, default=4, help='number of output messages per epoch')\r\n",
        "# Number of epochs to save a checkpoint\r\n",
        "parser.add_argument('--checkpoint_epochs', type=int, default=50, help='number of epochs to save a checkpoint')\r\n",
        "# Stop training after checkpoint is saved. Useful in long trainings\r\n",
        "parser.add_argument('--stop_after_checkpoint', type=bool, default=True, help='stop training after a checkpoint has been saved')\r\n",
        "\r\n",
        "# As stated in https://stackoverflow.com/questions/48796169/how-to-fix-ipykernel-launcher-py-error-unrecognized-arguments-in-jupyter\r\n",
        "# at least an empty list must be passed to simulate a script execution with no parameters.\r\n",
        "# If no parameter is provided, parse_args tries to read _sys.argv[1:], which is not defined\r\n",
        "# in a colab execution\r\n",
        "training_args = ['--cuda',\r\n",
        "                 '--epoch_count=151',\r\n",
        "                 '--niter=250',\r\n",
        "                 '--niter_decay=250',\r\n",
        "                 '--lr=0.002',\r\n",
        "                 '--lamb=1',\r\n",
        "                 '--direction=a2b',\r\n",
        "                 '--batch_size=5',\r\n",
        "                 '--checkpoint_epochs=25',\r\n",
        "                 '--threads=0',\r\n",
        "                 '--debug=1']\r\n",
        "opt = parser.parse_args(training_args)\r\n",
        "\r\n",
        "train_dir = 'dataset/train'\r\n",
        "train_gt_dir = train_dir + '/gt'\r\n",
        "train_images_dir = train_dir + '/images'\r\n",
        "train_tensorboard_dir = train_dir + '/log'\r\n",
        "\r\n",
        "test_dir = 'dataset/test'\r\n",
        "test_gt_dir = test_dir + '/gt'\r\n",
        "test_images_dir = test_dir + '/images'\r\n",
        "test_tensorboard_dir = test_dir + '/log'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rFyZk4__PvK"
      },
      "source": [
        "if opt.cuda and not torch.cuda.is_available():\r\n",
        "    raise Exception(\"No GPU found, please run without --cuda\")\r\n",
        "\r\n",
        "cudnn.benchmark = True\r\n",
        "\r\n",
        "torch.manual_seed(opt.seed)\r\n",
        "if opt.cuda:\r\n",
        "    torch.cuda.manual_seed(opt.seed)\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgmec5oC_2ps"
      },
      "source": [
        "### Debug function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSno8WYC_4ir"
      },
      "source": [
        "def print_debug(level, text):\r\n",
        "    \"\"\"\r\n",
        "    Prints a debug message only if the level of the message is lower or equal\r\n",
        "    to the debug level set in global variable debug\r\n",
        "    \"\"\"\r\n",
        "    # Accessing the global debug variable\r\n",
        "    # global debug\r\n",
        "    # The text will only be\r\n",
        "    if level <= opt.debug:\r\n",
        "        print(\"  [DEBUG] \" + text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-QxR1np9gFi"
      },
      "source": [
        "## Accessing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHU6-BqR_4Ej"
      },
      "source": [
        "### Defining the dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7lfv2t09lDs"
      },
      "source": [
        "# from utils import is_image_file, load_img\r\n",
        "def is_image_file(filename):\r\n",
        "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\"])\r\n",
        "\r\n",
        "# Used to save memory and CPU. Only valid with INRIA Aerial Image Dataset\r\n",
        "# All dataset tensors are in CPU by default\r\n",
        "zeros_tensor = torch.zeros([3,5000,5000]).to('cpu')\r\n",
        "\r\n",
        "class DatasetFromFolder(data.Dataset):\r\n",
        "    def __init__(self, image_dir, direction=\"a2b\"):\r\n",
        "        \"\"\"\r\n",
        "        Constructor adapted to the characteristics of the https://project.inria.fr/aerialimagelabeling/\r\n",
        "        images split as follows:\r\n",
        "        - train/a: training mask (ground truth) images\r\n",
        "        - train/b: training satellite images\r\n",
        "        - test/a: test mask (ground truth) images\r\n",
        "        - test/b: test satellite images\r\n",
        "\r\n",
        "        Example of use:\r\n",
        "        train_ds = DatasetFromFolder(\"/content/drive/MyDrive/Colab Notebooks/AIDL/Project/train\", \"a2b\")\r\n",
        "        \"\"\"\r\n",
        "        super(DatasetFromFolder, self).__init__()\r\n",
        "        self.direction = direction\r\n",
        "        self.a_path = join(image_dir, \"gt\")  # mask (ground truth) images. Originally \"a\"\r\n",
        "        self.b_path = join(image_dir, \"images\")  # satellite images. Originally \"b\"\r\n",
        "        self.image_filenames = [x for x in listdir(self.a_path) if is_image_file(x)]\r\n",
        "\r\n",
        "        transform_list = [transforms.ToTensor(),\r\n",
        "                          # Even if masks have only one channel, they're converted to RGB in __getitem__\r\n",
        "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\r\n",
        "        self.transform = transforms.Compose(transform_list)\r\n",
        "        \r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        filename = self.image_filenames[index]\r\n",
        "        print_debug(2, \"DatasetFromFolder __getitem__: getting item {} corresponding to file {}\".format(index, filename))\r\n",
        "        \r\n",
        "        a = Image.open(join(self.a_path, self.image_filenames[index])).convert('RGB')\r\n",
        "        b = Image.open(join(self.b_path, self.image_filenames[index])).convert('RGB')\r\n",
        "\r\n",
        "        # Originals can't be stored entirely in memory for later reuse because they take\r\n",
        "        # too much memory. So only a few will be stored\r\n",
        "        # Remember to avoid converting and storing them when doing the definitive training\r\n",
        "        \r\n",
        "        # Only one original image will be returned and, thus, transformed\r\n",
        "        # into Tensors. Converting all the original images is way too slow, and\r\n",
        "        # storing them in memory is useless as the DataTrainer creates a new instance of \r\n",
        "        # DatasetFromFolder on every epoch\r\n",
        "        if opt.tb_active and filename == opt.tb_image:\r\n",
        "            a_original = transforms.ToTensor()(a)\r\n",
        "            b_original = transforms.ToTensor()(b)\r\n",
        "            print_debug(2, \"DatasetFromFolder __getitem__: {} detected\".format(opt.tb_image))\r\n",
        "        else:\r\n",
        "            # Data loaders expect always the same amount of parameters and shapes when\r\n",
        "            # batch_size > 1\r\n",
        "            # \"RuntimeError: stack expects each tensor to be equal size, but got [1] at entry 0 and [3, 5000, 5000] at entry 7\"\r\n",
        "            a_original = zeros_tensor # torch.zeros([3] + list(a.size))\r\n",
        "            b_original = zeros_tensor # torch.zeros([3,5000,5000])\r\n",
        "\r\n",
        "        a = a.resize((286, 286), Image.BICUBIC) # Revision pending: from 5000x5000 to 286x286 sizes. This can lead to learning problems\r\n",
        "        b = b.resize((286, 286), Image.BICUBIC)\r\n",
        "        a = transforms.ToTensor()(a)\r\n",
        "        b = transforms.ToTensor()(b)\r\n",
        "\r\n",
        "        w_offset = random.randint(0, max(0, 286 - 256 - 1)) # \r\n",
        "        h_offset = random.randint(0, max(0, 286 - 256 - 1))\r\n",
        "    \r\n",
        "        a = a[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\r\n",
        "        b = b[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\r\n",
        "    \r\n",
        "        # After converting to RGB, even masks have 3 channels\r\n",
        "            # La transformación inversa sería simplemente min( (x*0.5)+0.5), 1)\r\n",
        "            # (haciendo un clipping de los valores para que no nos salgan colores raros).\r\n",
        "            # Tensorboard creo que ya gestiona lo del clipping;\r\n",
        "            # pero viene de nuestra cuenta hacer la \"desnormalización\".\r\n",
        "        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\r\n",
        "        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\r\n",
        "\r\n",
        "        if random.random() < 0.5:\r\n",
        "            idx = [i for i in range(a.size(2) - 1, -1, -1)]\r\n",
        "            idx = torch.LongTensor(idx)\r\n",
        "            a = a.index_select(2, idx)\r\n",
        "            b = b.index_select(2, idx)\r\n",
        "\r\n",
        "        if self.direction == \"a2b\":\r\n",
        "            return a, b, filename, b_original # Adding the original target image\r\n",
        "        else:\r\n",
        "            return b, a, filename, a_original # Adding the original target image\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.image_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmyzj3UIA3FS"
      },
      "source": [
        "### Connecting to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXmOv_spA5Fp",
        "outputId": "2aa826a2-1986-46ec-f890-3681b863ab2a"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfJxcVIjy0C_"
      },
      "source": [
        "### Copying files from Drive to CoLab machine\r\n",
        "\r\n",
        "According to this [article](https://enjoymachinelearning.com/posts/colab-with-google-drive/), reading files from the local storage in Google Colab is faster than doing so from Google Drive. Since all the training files are read once per epoch, it makes sense to copy them and work in the local filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc2i8PLfzADE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84338270-b700-487f-f210-77f46e023027"
      },
      "source": [
        "# Create directories where training and test images will be copied from Drive\r\n",
        "os.makedirs(name = train_gt_dir, exist_ok=True)\r\n",
        "os.makedirs(name = train_images_dir, exist_ok=True)\r\n",
        "# os.makedirs(name = train_tensorboard_dir, exist_ok=True)\r\n",
        "os.makedirs(name = test_gt_dir, exist_ok=True)\r\n",
        "os.makedirs(name = test_images_dir, exist_ok=True)\r\n",
        "# os.makedirs(name = test_tensorboard_dir, exist_ok=True)\r\n",
        "\r\n",
        "# Just in case, all files are deleted\r\n",
        "!rm \"{train_gt_dir}\"/*\r\n",
        "!rm \"{train_images_dir}\"/*\r\n",
        "!rm \"{test_gt_dir}\"/*\r\n",
        "!rm \"{test_images_dir}\"/*\r\n",
        "\r\n",
        "# Copy files ending in 0, 1, 2 or 3 to train directories. Shell commands don't accept python variables?\r\n",
        "!cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/train/gt/*[0-5].tif \"{train_gt_dir}\"\r\n",
        "# !cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/train/gt/*1.tif \"{train_gt_dir}\"\r\n",
        "!cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/train/images/*[0-5].tif \"{train_images_dir}\"\r\n",
        "# !cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/train/images/*1.tif \"{train_images_dir}\"\r\n",
        "\r\n",
        "# Copy files ending in 9 as test images\r\n",
        "!cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/train/gt/*9.tif \"{test_gt_dir}\"\r\n",
        "!cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/train/images/*9.tif \"{test_images_dir}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'dataset/train/gt/*': No such file or directory\n",
            "rm: cannot remove 'dataset/train/images/*': No such file or directory\n",
            "rm: cannot remove 'dataset/test/gt/*': No such file or directory\n",
            "rm: cannot remove 'dataset/test/images/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r7dXnK3AScL"
      },
      "source": [
        "### Creating the data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A21ZTd7AWCx"
      },
      "source": [
        "train_set = DatasetFromFolder(train_dir, opt.direction) # a2b is \"gt\" to \"images\"\r\n",
        "test_set  = DatasetFromFolder(test_dir, opt.direction)  # b2a is \"images\" to \"gt\"\r\n",
        "training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batch_size, shuffle=True)\r\n",
        "testing_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKE6KGPy-I7z"
      },
      "source": [
        "## Defining the networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_tQtrTvD_9v"
      },
      "source": [
        "### get_norm_layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtH0aSdlEEeK"
      },
      "source": [
        "def get_norm_layer(norm_type='instance'):\r\n",
        "    if norm_type == 'batch':\r\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\r\n",
        "    elif norm_type == 'instance':\r\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\r\n",
        "    elif norm_type == 'switchable':\r\n",
        "        norm_layer = SwitchNorm2d\r\n",
        "    elif norm_type == 'none':\r\n",
        "        norm_layer = None\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\r\n",
        "    return norm_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eadWfzY8EH47"
      },
      "source": [
        "### get_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCCLVUCNELDi"
      },
      "source": [
        "def get_scheduler(optimizer, opt):\r\n",
        "    if opt.lr_policy == 'lambda':\r\n",
        "        def lambda_rule(epoch):\r\n",
        "            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\r\n",
        "            return lr_l\r\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\r\n",
        "    elif opt.lr_policy == 'step':\r\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\r\n",
        "    elif opt.lr_policy == 'plateau':\r\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\r\n",
        "    elif opt.lr_policy == 'cosine':\r\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\r\n",
        "    else:\r\n",
        "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\r\n",
        "    return scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-s3i7BqERIS"
      },
      "source": [
        "### update_learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zZuU8L-ETRU"
      },
      "source": [
        "# update learning rate (called once every epoch)\r\n",
        "def update_learning_rate(scheduler, optimizer):\r\n",
        "    scheduler.step()\r\n",
        "    lr = optimizer.param_groups[0]['lr']\r\n",
        "    print('learning rate = %.7f' % lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkGcPIL1EbNh"
      },
      "source": [
        "### init_weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIq4Rj0hEcfK"
      },
      "source": [
        "def init_weights(net, init_type='normal', gain=0.02):\r\n",
        "    def init_func(m):\r\n",
        "        classname = m.__class__.__name__\r\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\r\n",
        "            if init_type == 'normal':\r\n",
        "                init.normal_(m.weight.data, 0.0, gain)\r\n",
        "            elif init_type == 'xavier':\r\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\r\n",
        "            elif init_type == 'kaiming':\r\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\r\n",
        "            elif init_type == 'orthogonal':\r\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\r\n",
        "            else:\r\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\r\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\r\n",
        "                init.constant_(m.bias.data, 0.0)\r\n",
        "        elif classname.find('BatchNorm2d') != -1:\r\n",
        "            init.normal_(m.weight.data, 1.0, gain)\r\n",
        "            init.constant_(m.bias.data, 0.0)\r\n",
        "\r\n",
        "    print('initialize network with %s' % init_type)\r\n",
        "    net.apply(init_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIcDgte_EiYR"
      },
      "source": [
        "### init_net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRuLD2VaEkOC"
      },
      "source": [
        "def init_net(net, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\r\n",
        "    net.to(gpu_id)\r\n",
        "    init_weights(net, init_type, gain=init_gain)\r\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Zx0lQCEnVv"
      },
      "source": [
        "### define_G"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn_85WqnEpb5"
      },
      "source": [
        "def define_G(input_nc, output_nc, ngf, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\r\n",
        "    net = None\r\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\r\n",
        "\r\n",
        "    net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\r\n",
        "   \r\n",
        "    return init_net(net, init_type, init_gain, gpu_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRql8ZFgEtwr"
      },
      "source": [
        "### Class ResnetGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a17K64xEwgC"
      },
      "source": [
        "# Defines the generator that consists of Resnet blocks between a few\r\n",
        "# downsampling/upsampling operations.\r\n",
        "class ResnetGenerator(nn.Module):\r\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=9, padding_type='reflect'):\r\n",
        "        assert(n_blocks >= 0)\r\n",
        "        super(ResnetGenerator, self).__init__()\r\n",
        "        self.input_nc = input_nc\r\n",
        "        self.output_nc = output_nc\r\n",
        "        self.ngf = ngf\r\n",
        "        if type(norm_layer) == functools.partial:\r\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\r\n",
        "        else:\r\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\r\n",
        "\r\n",
        "        self.inc = Inconv(input_nc, ngf, norm_layer, use_bias)\r\n",
        "        self.down1 = Down(ngf, ngf * 2, norm_layer, use_bias)\r\n",
        "        self.down2 = Down(ngf * 2, ngf * 4, norm_layer, use_bias)\r\n",
        "\r\n",
        "        model = []\r\n",
        "        for i in range(n_blocks):\r\n",
        "            model += [ResBlock(ngf * 4, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\r\n",
        "        self.resblocks = nn.Sequential(*model)\r\n",
        "\r\n",
        "        self.up1 = Up(ngf * 4, ngf * 2, norm_layer, use_bias)\r\n",
        "        self.up2 = Up(ngf * 2, ngf, norm_layer, use_bias)\r\n",
        "\r\n",
        "        self.outc = Outconv(ngf, output_nc)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        out = {}\r\n",
        "        # DTT No hay skip connections?\r\n",
        "        out['in'] = self.inc(input)\r\n",
        "        out['d1'] = self.down1(out['in'])\r\n",
        "        out['d2'] = self.down2(out['d1'])\r\n",
        "        out['bottle'] = self.resblocks(out['d2'])\r\n",
        "        out['u1'] = self.up1(out['bottle'])\r\n",
        "        out['u2'] = self.up2(out['u1'])\r\n",
        "\r\n",
        "        return self.outc(out['u2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G3-634DE1Na"
      },
      "source": [
        "### Class Inconv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek0MJpq5E3GL"
      },
      "source": [
        "class Inconv(nn.Module):\r\n",
        "    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\r\n",
        "        super(Inconv, self).__init__()\r\n",
        "        self.inconv = nn.Sequential(\r\n",
        "            nn.ReflectionPad2d(3),\r\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=7, padding=0,\r\n",
        "                      bias=use_bias),\r\n",
        "            norm_layer(out_ch),\r\n",
        "            nn.ReLU(True)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.inconv(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ReF-Na4E7Ca"
      },
      "source": [
        "Class Down"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tulCKo7E9Cr"
      },
      "source": [
        "class Down(nn.Module):\r\n",
        "    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\r\n",
        "        super(Down, self).__init__()\r\n",
        "        self.down = nn.Sequential(\r\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3,\r\n",
        "                      stride=2, padding=1, bias=use_bias),\r\n",
        "            norm_layer(out_ch),\r\n",
        "            nn.ReLU(True)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.down(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTaAiirPFA1L"
      },
      "source": [
        "### Class ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gzyti8rFC-6"
      },
      "source": [
        "# Define a Resnet block\r\n",
        "class ResBlock(nn.Module):\r\n",
        "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\r\n",
        "        super(ResBlock, self).__init__()\r\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\r\n",
        "\r\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\r\n",
        "        conv_block = []\r\n",
        "        p = 0\r\n",
        "        if padding_type == 'reflect':\r\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\r\n",
        "        elif padding_type == 'replicate':\r\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\r\n",
        "        elif padding_type == 'zero':\r\n",
        "            p = 1\r\n",
        "        else:\r\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\r\n",
        "\r\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\r\n",
        "                       norm_layer(dim),\r\n",
        "                       nn.ReLU(True)]\r\n",
        "        if use_dropout:\r\n",
        "            conv_block += [nn.Dropout(0.5)]\r\n",
        "\r\n",
        "        p = 0\r\n",
        "        if padding_type == 'reflect':\r\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\r\n",
        "        elif padding_type == 'replicate':\r\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\r\n",
        "        elif padding_type == 'zero':\r\n",
        "            p = 1\r\n",
        "        else:\r\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\r\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\r\n",
        "                       norm_layer(dim)]\r\n",
        "\r\n",
        "        return nn.Sequential(*conv_block)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # DTT 'x +' === skip connection!!\r\n",
        "        out = x + self.conv_block(x)\r\n",
        "        return nn.ReLU(True)(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf5OAy96FQkq"
      },
      "source": [
        "### Class Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCUVoDe5FSYt"
      },
      "source": [
        "class Up(nn.Module):\r\n",
        "    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\r\n",
        "        super(Up, self).__init__()\r\n",
        "        self.up = nn.Sequential(\r\n",
        "            # nn.Upsample(scale_factor=2, mode='nearest'),\r\n",
        "            # nn.Conv2d(in_ch, out_ch,\r\n",
        "            #           kernel_size=3, stride=1,\r\n",
        "            #           padding=1, bias=use_bias),\r\n",
        "            nn.ConvTranspose2d(in_ch, out_ch,\r\n",
        "                               kernel_size=3, stride=2,\r\n",
        "                               padding=1, output_padding=1,\r\n",
        "                               bias=use_bias),\r\n",
        "            norm_layer(out_ch),\r\n",
        "            nn.ReLU(True)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.up(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNeXjgOPFWj2"
      },
      "source": [
        "### Class Outconv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtKFFDlhFclO"
      },
      "source": [
        "class Outconv(nn.Module):\r\n",
        "    def __init__(self, in_ch, out_ch):\r\n",
        "        super(Outconv, self).__init__()\r\n",
        "        self.outconv = nn.Sequential(\r\n",
        "            nn.ReflectionPad2d(3),\r\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=7, padding=0),\r\n",
        "            nn.Tanh()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.outconv(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqQHEbYLFi9D"
      },
      "source": [
        "### define_D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wade_AR1Fk6W"
      },
      "source": [
        "def define_D(input_nc, ndf, netD,\r\n",
        "             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\r\n",
        "    net = None\r\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\r\n",
        "\r\n",
        "    if netD == 'basic':\r\n",
        "        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\r\n",
        "    elif netD == 'n_layers':\r\n",
        "        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\r\n",
        "    elif netD == 'pixel':\r\n",
        "        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('Discriminator model name [%s] is not recognized' % net)\r\n",
        "\r\n",
        "    return init_net(net, init_type, init_gain, gpu_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJEbSaM4Fu3b"
      },
      "source": [
        "### Class NLayerDiscriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13bSCt9BFz2m"
      },
      "source": [
        "# Defines the PatchGAN discriminator with the specified arguments.\r\n",
        "class NLayerDiscriminator(nn.Module):\r\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\r\n",
        "        super(NLayerDiscriminator, self).__init__()\r\n",
        "        if type(norm_layer) == functools.partial:\r\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\r\n",
        "        else:\r\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\r\n",
        "\r\n",
        "        kw = 4\r\n",
        "        padw = 1\r\n",
        "        sequence = [\r\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\r\n",
        "            nn.LeakyReLU(0.2, True)\r\n",
        "        ]\r\n",
        "\r\n",
        "        nf_mult = 1\r\n",
        "        nf_mult_prev = 1\r\n",
        "        for n in range(1, n_layers):\r\n",
        "            nf_mult_prev = nf_mult\r\n",
        "            nf_mult = min(2**n, 8)\r\n",
        "            sequence += [\r\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\r\n",
        "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\r\n",
        "                norm_layer(ndf * nf_mult),\r\n",
        "                nn.LeakyReLU(0.2, True)\r\n",
        "            ]\r\n",
        "\r\n",
        "        nf_mult_prev = nf_mult\r\n",
        "        nf_mult = min(2**n_layers, 8)\r\n",
        "        sequence += [\r\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\r\n",
        "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\r\n",
        "            norm_layer(ndf * nf_mult),\r\n",
        "            nn.LeakyReLU(0.2, True)\r\n",
        "        ]\r\n",
        "\r\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\r\n",
        "\r\n",
        "        if use_sigmoid:\r\n",
        "            sequence += [nn.Sigmoid()]\r\n",
        "\r\n",
        "        self.model = nn.Sequential(*sequence)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return self.model(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMk0O6ZWF6fN"
      },
      "source": [
        "### Class PixelDiscriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dniCAgHxF8pK"
      },
      "source": [
        "class PixelDiscriminator(nn.Module):\r\n",
        "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\r\n",
        "        super(PixelDiscriminator, self).__init__()\r\n",
        "        if type(norm_layer) == functools.partial:\r\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\r\n",
        "        else:\r\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\r\n",
        "\r\n",
        "        self.net = [\r\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\r\n",
        "            norm_layer(ndf * 2),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\r\n",
        "\r\n",
        "        if use_sigmoid:\r\n",
        "            self.net.append(nn.Sigmoid())\r\n",
        "\r\n",
        "        self.net = nn.Sequential(*self.net)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return self.net(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fps3rv0eGA5Z"
      },
      "source": [
        "### Class GANLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm8N57rx-K4p"
      },
      "source": [
        "class GANLoss(nn.Module):\r\n",
        "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\r\n",
        "        super(GANLoss, self).__init__()\r\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\r\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\r\n",
        "        if use_lsgan:\r\n",
        "            self.loss = nn.MSELoss()\r\n",
        "        else:\r\n",
        "            self.loss = nn.BCELoss()\r\n",
        "\r\n",
        "    def get_target_tensor(self, input, target_is_real):\r\n",
        "        if target_is_real:\r\n",
        "            target_tensor = self.real_label\r\n",
        "        else:\r\n",
        "            target_tensor = self.fake_label\r\n",
        "        return target_tensor.expand_as(input)\r\n",
        "\r\n",
        "    def __call__(self, input, target_is_real):\r\n",
        "        target_tensor = self.get_target_tensor(input, target_is_real)\r\n",
        "        return self.loss(input, target_tensor)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6HwOMTGYQtX"
      },
      "source": [
        "### Creating the networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO6RpCjdYUgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c9d800-42f4-4e1c-c8ea-028f652b68f1"
      },
      "source": [
        "# Creating the networks from scratch\r\n",
        "net_g = define_G(opt.input_nc, opt.output_nc, opt.ngf, 'batch', False, 'normal', 0.02, gpu_id=device)\r\n",
        "net_d = define_D(opt.input_nc + opt.output_nc, opt.ndf, 'basic', gpu_id=device)\r\n",
        "\r\n",
        "criterionGAN = GANLoss().to(device)\r\n",
        "criterionL1 = nn.L1Loss().to(device)\r\n",
        "criterionMSE = nn.MSELoss().to(device)\r\n",
        "\r\n",
        "# setup optimizer\r\n",
        "optimizer_g = optim.Adam(net_g.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\r\n",
        "optimizer_d = optim.Adam(net_d.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\r\n",
        "net_g_scheduler = get_scheduler(optimizer_g, opt)\r\n",
        "net_d_scheduler = get_scheduler(optimizer_d, opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialize network with normal\n",
            "initialize network with normal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7kQa3wkOrsk"
      },
      "source": [
        "## Auxiliary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRQHgNJVSaOv"
      },
      "source": [
        "### denormalize_image & show_image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjmrRT0ESdyO"
      },
      "source": [
        "def denormalize_image(image_tensor):\r\n",
        "    \"\"\"\r\n",
        "    Denormalizes an image coming from the network, usually, a generated image\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    images_tensor: tensor representing a PIL image\r\n",
        "    \"\"\"\r\n",
        "    print_debug(2, \"denormalize_image image tensor shape: {}\".format(image_tensor.shape))\r\n",
        "    # cpu() to avoid error \"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\"\r\n",
        "    image_numpy = image_tensor.cpu().data.float().numpy()\r\n",
        "    \r\n",
        "            # La transformación inversa sería simplemente min( (x*0.5)+0.5), 1)\r\n",
        "            # (haciendo un clipping de los valores para que no nos salgan colores raros).\r\n",
        "            # Tensorboard creo que ya gestiona lo del clipping;\r\n",
        "            # pero viene de nuestra cuenta hacer la \"desnormalización\".\r\n",
        "\r\n",
        "    print_debug(2, \"denormalize_image image_numpy shape: {}\".format(image_numpy.shape))\r\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\r\n",
        "    print_debug(2, \"denormalize_image image_numpy shape: {} after transposing\".format(image_numpy.shape))\r\n",
        "    image_numpy = image_numpy.clip(0, 255)\r\n",
        "    print_debug(2, \"denormalize_image image_numpy shape: {} after clipping\".format(image_numpy.shape))\r\n",
        "    image_numpy = image_numpy.astype(np.uint8)\r\n",
        "    print_debug(2, \"denormalize_image image_numpy shape: {} after converting to uint8\".format(image_numpy.shape))\r\n",
        "\r\n",
        "    return image_numpy\r\n",
        "\r\n",
        "def show_image(image_tensor):\r\n",
        "    \"\"\"\r\n",
        "    Shows an image coming from the network\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    \"\"\"\r\n",
        "    image_numpy = denormalize_image(image_tensor)\r\n",
        "    pil_image = Image.fromarray(image_numpy)\r\n",
        "    imshow(pil_image)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmhnfzo8p2wf"
      },
      "source": [
        "### Show list/tuple of images in a grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx7fIkGUp9j6"
      },
      "source": [
        "# Based on utils.py save_img and the last answer in\r\n",
        "# https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly/46616645#46616645\r\n",
        "# Plots several figures in a tile\r\n",
        "def show_images_grid(images_tuple, nrows=1, ncols=1):\r\n",
        "    \"\"\"\r\n",
        "    Shows several images coming from a DataLoader based on DatasetFromFolder\r\n",
        "    in a tile\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    images_tuple: tuple of tensors representing images\r\n",
        "    ncols : number of columns of subplots wanted in the display\r\n",
        "    nrows : number of rows of subplots wanted in the figure\r\n",
        "    \"\"\"\r\n",
        "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows, figsize=(15,15))\r\n",
        "    for ind,image_tensor in zip(range(len(images_tuple)), images_tuple):\r\n",
        "        # First, denormalize image to allow it to be printable\r\n",
        "        image_numpy = denormalize_image(image_tensor)\r\n",
        "        image_pil = Image.fromarray(image_numpy)\r\n",
        "        # imshow(image_pil)\r\n",
        "        \r\n",
        "        axeslist.ravel()[ind].imshow(image_pil, cmap=plt.jet())\r\n",
        "        # axeslist.ravel()[ind].set_title(title)\r\n",
        "        axeslist.ravel()[ind].set_axis_off()\r\n",
        "    plt.tight_layout() # optional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7gO9YJxwIjV"
      },
      "source": [
        "### setup_tensorboard_writer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p8NPbl6wRn8"
      },
      "source": [
        "def setup_tensorboard_writer(tensorboard_dir, model=None):\r\n",
        "    \"\"\"\r\n",
        "    Creates a new directory in tensorboard_dir to log data for TensorBoard.\r\n",
        "    If a model/net is provided, it is added to the writer.\r\n",
        "\r\n",
        "    Returns a reference to the writer\r\n",
        "    \"\"\"\r\n",
        "    # Setting up TensorBoard writer\r\n",
        "    # Creates a new directory to store TensorBoard data\r\n",
        "    log_subdir = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "    writer = SummaryWriter(log_dir=tensorboard_dir + \"/\" + log_subdir)\r\n",
        "\r\n",
        "    if model is not None:\r\n",
        "        # Adding the model to TensorBoard\r\n",
        "        # Apparently, TensorBoard only accepts one model per writer\r\n",
        "        writer.add_graph(model, input_to_model=torch.randn([1,3,256,256]).to(device))\r\n",
        "\r\n",
        "    return writer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uG68lfo0HZf"
      },
      "source": [
        "### save_iteration_tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWemRuem0OR1"
      },
      "source": [
        "def save_iteration_tensorboard(writer, epoch, iteration, loss_d, loss_g, loss_g_gan, loss_g_l1,\r\n",
        "                               real_a, real_b, fake_b, batch):\r\n",
        "    tensorboard_step = len(training_data_loader.dataset.image_filenames) * (epoch - opt.epoch_count) + iteration\r\n",
        "    writer.add_scalar('Loss/D', loss_d.item(), global_step=tensorboard_step)\r\n",
        "    writer.add_scalar('Loss/G', loss_g.item(), global_step=tensorboard_step)\r\n",
        "    writer.add_scalar('Loss/G GAN', loss_g_gan.data, global_step=tensorboard_step)\r\n",
        "    writer.add_scalar('Loss/G L1', loss_g_l1.data, global_step=tensorboard_step)\r\n",
        "\r\n",
        "    # DTT Decide whether saving images to tensorboard\r\n",
        "    final_epoch = opt.niter + opt.niter_decay + 1\r\n",
        "                                # final_epoch / opt.tb_number_img gives the number of epochs\r\n",
        "                                # that should pass before an image is saved.\r\n",
        "    epochs_to_pass = max(1, final_epoch // opt.tb_number_img) # at least should be 1\r\n",
        "    save_image_to_tensorboard = ( ( (epoch % epochs_to_pass == 0)\r\n",
        "                                # or it is the last epoch of training\r\n",
        "                                    or (epoch == final_epoch)\r\n",
        "                                  )\r\n",
        "                                  # it only saves the image if it corresponds to the defined opt.tb_image\r\n",
        "                                  and opt.tb_image in batch[2]\r\n",
        "    )\r\n",
        "    if save_image_to_tensorboard:\r\n",
        "        print_debug(2, \"save_iteration_tensorboard: saving {} to TensorBoard. Is in? {}. Batch: {}\".format(opt.tb_image, opt.tb_image in batch[2], batch[2]))\r\n",
        "        \r\n",
        "        batch_index = batch[2].index(opt.tb_image)\r\n",
        "        # DTT Write images to TensorBoard at the end of each epoch\r\n",
        "        writer.add_image(str(epoch)+'/1 Mask', real_a[batch_index], epoch)\r\n",
        "        writer.add_image(str(epoch)+'/2 Normalized satellite image', real_b[batch_index], epoch)\r\n",
        "        writer.add_image(str(epoch)+'/3 Generated satellite image', fake_b[batch_index], epoch)\r\n",
        "        writer.add_image(str(epoch)+'/4 Denormalized generated satellite image', denormalize_image(fake_b[batch_index]), epoch, dataformats='HWC')\r\n",
        "        writer.add_image(str(epoch)+'/5 Original satellite image', batch[3][batch_index].squeeze(dim=0), epoch)\r\n",
        "    else:\r\n",
        "        print_debug(2, \"save_iteration_tensorboard: won't save any image ({})\".format(batch[2]))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSHF47QatVcN"
      },
      "source": [
        "### save_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ROmgmG1tXmw"
      },
      "source": [
        "def save_checkpoint(epoch, net_g, net_d):\r\n",
        "    \"\"\"\r\n",
        "    Saves the discriminator and generator.\r\n",
        "    It returns a boolean stating whether training should stop or not\r\n",
        "    \"\"\"\r\n",
        "    if epoch % opt.checkpoint_epochs == 0:\r\n",
        "        os.makedirs(name = \"checkpoint\", exist_ok=True)\r\n",
        "        net_g_model_out_path = \"checkpoint/netG_model_epoch_{}.pth\".format(epoch)\r\n",
        "        net_d_model_out_path = \"checkpoint/netD_model_epoch_{}.pth\".format(epoch)\r\n",
        "        torch.save(net_g, net_g_model_out_path)\r\n",
        "        torch.save(net_d, net_d_model_out_path)\r\n",
        "        print(\"Checkpoint for epoch {} saved\".format(epoch))\r\n",
        "\r\n",
        "        if opt.stop_after_checkpoint:\r\n",
        "            return True\r\n",
        "        else:\r\n",
        "            return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV5f5MC3vrCm"
      },
      "source": [
        "## Loading previous trained checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fhR6isyblXF",
        "outputId": "75da8ad9-3668-4474-a483-e0a81017bd5c"
      },
      "source": [
        "!ls drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/train/log/net*100.pth\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'drive/MyDrive/Colab Notebooks/AIDL/Project/train/log/net*100.pth': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEDth9GOWXQw"
      },
      "source": [
        "# Loading already calculated weights\r\n",
        "net_g = torch.load('drive/MyDrive/Colab Notebooks/AIDL/Project/train/trainedModels/netG_model_epoch_150.pth', map_location=torch.device(device)).to(device)\r\n",
        "net_d = torch.load('drive/MyDrive/Colab Notebooks/AIDL/Project/train/trainedModels/netD_model_epoch_150.pth', map_location=torch.device(device)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlxLU_Uj-Tkx"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmpY-pNqHhNd",
        "outputId": "a3b770d2-2029-4bae-8b9c-737a368f0a9a"
      },
      "source": [
        "if opt.tb_active:\r\n",
        "    writer_train = setup_tensorboard_writer(train_tensorboard_dir, model=net_g)\r\n",
        "\r\n",
        "output_images = []\r\n",
        "start_time = time.time()\r\n",
        "\r\n",
        "# Training function\r\n",
        "for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\r\n",
        "    epoch_start_time = time.time()\r\n",
        "    # train\r\n",
        "    for iteration, batch in enumerate(training_data_loader, 1):\r\n",
        "        # forward\r\n",
        "        # DTT a: masks, b: satellite image\r\n",
        "        real_a, real_b = batch[0].to(device), batch[1].to(device)\r\n",
        "        # DTT fake_b: generated satellite image from generator\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** self.fake_B = self.netG(self.real_A)  # G(A)\r\n",
        "        fake_b = net_g(real_a)\r\n",
        "\r\n",
        "        ######################\r\n",
        "        # (1) Update D network\r\n",
        "        ######################\r\n",
        "\r\n",
        "        optimizer_d.zero_grad()\r\n",
        "        \r\n",
        "        # train with fake\r\n",
        "        # DTT Concatenates the real mask with generated image\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\r\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\r\n",
        "\r\n",
        "        # DTT Discriminator's prediction stating if the couple of images are\r\n",
        "        #     (real, real) o (real, false)\r\n",
        "        #     detach() to avoid calculating gradients\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** pred_fake = self.netD(fake_AB.detach())\r\n",
        "        #     ** # Fake; stop backprop to the generator by detaching fake_B\r\n",
        "        pred_fake = net_d.forward(fake_ab.detach())\r\n",
        "\r\n",
        "        # DTT Calculated losses where extremely big. Debug message to see why\r\n",
        "        print_debug(2, \"Train: pred_fake's shape {}, min {} and max {}\".format(\r\n",
        "            pred_fake.shape, pred_fake.min(), pred_fake.max()\r\n",
        "        ))\r\n",
        "\r\n",
        "        # DTT Loss when a generated image is fed. Should classificate it as False\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** self.loss_D_fake = self.criterionGAN(pred_fake, False)\r\n",
        "        loss_d_fake = criterionGAN(pred_fake, False)\r\n",
        "\r\n",
        "        # train with real\r\n",
        "        # DTT Concatenates the same real mask with its corresponding real image\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** real_AB = torch.cat((self.real_A, self.real_B), 1)\r\n",
        "        real_ab = torch.cat((real_a, real_b), 1)\r\n",
        "        # DTT Discriminator's prediction. Now calculating gradients\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** pred_real = self.netD(real_AB)\r\n",
        "        pred_real = net_d.forward(real_ab)\r\n",
        "        # DTT Discriminator should predict True with a real mask + image couple\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** self.loss_D_real = self.criterionGAN(pred_real, True)\r\n",
        "        loss_d_real = criterionGAN(pred_real, True)\r\n",
        "        \r\n",
        "        # Combined D loss\r\n",
        "        # DTT D's loss is the mean between its capacity ot detect a generated image\r\n",
        "        #     and its capacity to detect a real image\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** # combine loss and calculate gradients\r\n",
        "        #     ** self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\r\n",
        "        loss_d = (loss_d_fake + loss_d_real) * 0.5\r\n",
        "\r\n",
        "        loss_d.backward()\r\n",
        "       \r\n",
        "        optimizer_d.step()\r\n",
        "\r\n",
        "        ######################\r\n",
        "        # (2) Update G network\r\n",
        "        ######################\r\n",
        "\r\n",
        "        # DTT In the pix2pix original implementation, discriminator's gradients\r\n",
        "        #     are deactivated\r\n",
        "        #     ** self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\r\n",
        "\r\n",
        "        optimizer_g.zero_grad()\r\n",
        "\r\n",
        "        # First, G(A) should fake the discriminator\r\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\r\n",
        "        pred_fake = net_d.forward(fake_ab)\r\n",
        "        loss_g_gan = criterionGAN(pred_fake, True)\r\n",
        "\r\n",
        "        # Second, G(A) = B\r\n",
        "        loss_g_l1 = criterionL1(fake_b, real_b) * opt.lamb\r\n",
        "        loss_g = loss_g_gan + loss_g_l1\r\n",
        "        loss_g.backward()\r\n",
        "        optimizer_g.step()\r\n",
        "\r\n",
        "        # DTT Let's print just some iteration messages per epoch\r\n",
        "        #     Iterations go from 1 to ceiling(len(train_set) / batch_size)\r\n",
        "        if iteration % (math.ceil(len(train_set) / opt.batch_size) // opt.iter_messages) == 0:\r\n",
        "            print(\"===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\".format(\r\n",
        "                epoch, iteration, len(training_data_loader), loss_d.item(), loss_g.item()))\r\n",
        "        \r\n",
        "        # DTT Logging the same data for TensorBoard analysis\r\n",
        "        if opt.tb_active:\r\n",
        "            save_iteration_tensorboard(writer_train, epoch, iteration, loss_d, loss_g, loss_g_gan, loss_g_l1,\r\n",
        "                               real_a, real_b, fake_b, batch)\r\n",
        "\r\n",
        "    # Only execute if a minimum epochs are expected\r\n",
        "    if (opt.niter + opt.niter_decay + 1) > opt.checkpoint_epochs:\r\n",
        "        update_learning_rate(net_g_scheduler, optimizer_g)\r\n",
        "        update_learning_rate(net_d_scheduler, optimizer_d)\r\n",
        "\r\n",
        "    # test\r\n",
        "    avg_psnr = 0\r\n",
        "    for batch in testing_data_loader:\r\n",
        "        input, target = batch[0].to(device), batch[1].to(device)\r\n",
        "\r\n",
        "        prediction = net_g(input)\r\n",
        "        mse = criterionMSE(prediction, target)\r\n",
        "        psnr = 10 * log10(1 / mse.item())\r\n",
        "        avg_psnr += psnr\r\n",
        "    print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr / len(testing_data_loader)))\r\n",
        "\r\n",
        "    if opt.tb_active:\r\n",
        "        # DTT I log the same data for TensorBoard analysis\r\n",
        "        writer_train.add_scalar('Avg. PSNR', avg_psnr / len(testing_data_loader), epoch)\r\n",
        "        time_spent = time.time() - epoch_start_time\r\n",
        "        print_debug(1, \"Train: time spent in epoch {} is {}\".format(epoch, time_spent))\r\n",
        "        writer_train.add_scalar('Time spent', time_spent, epoch)\r\n",
        "\r\n",
        "    #checkpoint\r\n",
        "    exit = save_checkpoint(epoch, net_g, net_d)\r\n",
        "    if exit:\r\n",
        "        print(\"Ending training as stop_after_checkpoint is set to True\")\r\n",
        "        break\r\n",
        "\r\n",
        "if opt.tb_active:\r\n",
        "    writer_train.close()\r\n",
        "\r\n",
        "print(\"\\nTraining ended. It took {} seconds\".format(time.time() - start_time))\r\n",
        "print(\"Arguments used: {}\".format(training_args))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===> Epoch[151](5/23): Loss_D: 0.2396 Loss_G: 0.6186\n",
            "===> Epoch[151](10/23): Loss_D: 0.2452 Loss_G: 0.5928\n",
            "===> Epoch[151](15/23): Loss_D: 0.2147 Loss_G: 0.6151\n",
            "===> Epoch[151](20/23): Loss_D: 0.2105 Loss_G: 0.6141\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.1232 dB\n",
            "  [DEBUG] Train: time spent in epoch 151 is 357.28951358795166\n",
            "===> Epoch[152](5/23): Loss_D: 0.2018 Loss_G: 0.6396\n",
            "===> Epoch[152](10/23): Loss_D: 0.2221 Loss_G: 0.5794\n",
            "===> Epoch[152](15/23): Loss_D: 0.2529 Loss_G: 0.6416\n",
            "===> Epoch[152](20/23): Loss_D: 0.2125 Loss_G: 0.6634\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.3307 dB\n",
            "  [DEBUG] Train: time spent in epoch 152 is 333.3474531173706\n",
            "===> Epoch[153](5/23): Loss_D: 0.2330 Loss_G: 0.6917\n",
            "===> Epoch[153](10/23): Loss_D: 0.2194 Loss_G: 0.6565\n",
            "===> Epoch[153](15/23): Loss_D: 0.2339 Loss_G: 0.6787\n",
            "===> Epoch[153](20/23): Loss_D: 0.2189 Loss_G: 0.5972\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.1475 dB\n",
            "  [DEBUG] Train: time spent in epoch 153 is 356.54130935668945\n",
            "===> Epoch[154](5/23): Loss_D: 0.2253 Loss_G: 0.6291\n",
            "===> Epoch[154](10/23): Loss_D: 0.2322 Loss_G: 0.6245\n",
            "===> Epoch[154](15/23): Loss_D: 0.2260 Loss_G: 0.6562\n",
            "===> Epoch[154](20/23): Loss_D: 0.2478 Loss_G: 0.6178\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0717 dB\n",
            "  [DEBUG] Train: time spent in epoch 154 is 356.3464603424072\n",
            "===> Epoch[155](5/23): Loss_D: 0.2527 Loss_G: 0.5549\n",
            "===> Epoch[155](10/23): Loss_D: 0.2051 Loss_G: 0.5975\n",
            "===> Epoch[155](15/23): Loss_D: 0.2141 Loss_G: 0.6451\n",
            "===> Epoch[155](20/23): Loss_D: 0.2170 Loss_G: 0.6451\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 7.9861 dB\n",
            "  [DEBUG] Train: time spent in epoch 155 is 342.3646357059479\n",
            "===> Epoch[156](5/23): Loss_D: 0.2252 Loss_G: 0.6562\n",
            "===> Epoch[156](10/23): Loss_D: 0.2095 Loss_G: 0.6811\n",
            "===> Epoch[156](15/23): Loss_D: 0.2147 Loss_G: 0.6063\n",
            "===> Epoch[156](20/23): Loss_D: 0.2436 Loss_G: 0.6596\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.2323 dB\n",
            "  [DEBUG] Train: time spent in epoch 156 is 340.19105339050293\n",
            "===> Epoch[157](5/23): Loss_D: 0.2099 Loss_G: 0.6319\n",
            "===> Epoch[157](10/23): Loss_D: 0.2587 Loss_G: 0.6139\n",
            "===> Epoch[157](15/23): Loss_D: 0.2166 Loss_G: 0.6206\n",
            "===> Epoch[157](20/23): Loss_D: 0.2044 Loss_G: 0.6585\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0137 dB\n",
            "  [DEBUG] Train: time spent in epoch 157 is 341.19887375831604\n",
            "===> Epoch[158](5/23): Loss_D: 0.2309 Loss_G: 0.6252\n",
            "===> Epoch[158](10/23): Loss_D: 0.2221 Loss_G: 0.6101\n",
            "===> Epoch[158](15/23): Loss_D: 0.2381 Loss_G: 0.6171\n",
            "===> Epoch[158](20/23): Loss_D: 0.2184 Loss_G: 0.6788\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0737 dB\n",
            "  [DEBUG] Train: time spent in epoch 158 is 351.82951164245605\n",
            "===> Epoch[159](5/23): Loss_D: 0.2214 Loss_G: 0.6401\n",
            "===> Epoch[159](10/23): Loss_D: 0.2250 Loss_G: 0.6534\n",
            "===> Epoch[159](15/23): Loss_D: 0.2218 Loss_G: 0.6358\n",
            "===> Epoch[159](20/23): Loss_D: 0.2227 Loss_G: 0.6548\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0086 dB\n",
            "  [DEBUG] Train: time spent in epoch 159 is 335.9195728302002\n",
            "===> Epoch[160](5/23): Loss_D: 0.2196 Loss_G: 0.6091\n",
            "===> Epoch[160](10/23): Loss_D: 0.2201 Loss_G: 0.6141\n",
            "===> Epoch[160](15/23): Loss_D: 0.2511 Loss_G: 0.5882\n",
            "===> Epoch[160](20/23): Loss_D: 0.2206 Loss_G: 0.6029\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 7.9809 dB\n",
            "  [DEBUG] Train: time spent in epoch 160 is 356.5259168148041\n",
            "===> Epoch[161](5/23): Loss_D: 0.2266 Loss_G: 0.6127\n",
            "===> Epoch[161](10/23): Loss_D: 0.2162 Loss_G: 0.6430\n",
            "===> Epoch[161](15/23): Loss_D: 0.2121 Loss_G: 0.6546\n",
            "===> Epoch[161](20/23): Loss_D: 0.2081 Loss_G: 0.6721\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 7.9931 dB\n",
            "  [DEBUG] Train: time spent in epoch 161 is 354.6618673801422\n",
            "===> Epoch[162](5/23): Loss_D: 0.2386 Loss_G: 0.6356\n",
            "===> Epoch[162](10/23): Loss_D: 0.2385 Loss_G: 0.6658\n",
            "===> Epoch[162](15/23): Loss_D: 0.2199 Loss_G: 0.6592\n",
            "===> Epoch[162](20/23): Loss_D: 0.2379 Loss_G: 0.6349\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0734 dB\n",
            "  [DEBUG] Train: time spent in epoch 162 is 338.32465386390686\n",
            "===> Epoch[163](5/23): Loss_D: 0.2115 Loss_G: 0.6474\n",
            "===> Epoch[163](10/23): Loss_D: 0.2211 Loss_G: 0.6675\n",
            "===> Epoch[163](15/23): Loss_D: 0.2404 Loss_G: 0.6468\n",
            "===> Epoch[163](20/23): Loss_D: 0.2125 Loss_G: 0.6286\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 7.9679 dB\n",
            "  [DEBUG] Train: time spent in epoch 163 is 344.3288960456848\n",
            "===> Epoch[164](5/23): Loss_D: 0.2658 Loss_G: 0.5284\n",
            "===> Epoch[164](10/23): Loss_D: 0.2318 Loss_G: 0.6202\n",
            "===> Epoch[164](15/23): Loss_D: 0.2082 Loss_G: 0.6631\n",
            "===> Epoch[164](20/23): Loss_D: 0.2112 Loss_G: 0.6278\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0713 dB\n",
            "  [DEBUG] Train: time spent in epoch 164 is 332.198335647583\n",
            "===> Epoch[165](5/23): Loss_D: 0.2354 Loss_G: 0.6539\n",
            "===> Epoch[165](10/23): Loss_D: 0.2416 Loss_G: 0.6229\n",
            "===> Epoch[165](15/23): Loss_D: 0.2059 Loss_G: 0.6634\n",
            "===> Epoch[165](20/23): Loss_D: 0.2058 Loss_G: 0.5715\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0953 dB\n",
            "  [DEBUG] Train: time spent in epoch 165 is 355.4578948020935\n",
            "===> Epoch[166](5/23): Loss_D: 0.2111 Loss_G: 0.5961\n",
            "===> Epoch[166](10/23): Loss_D: 0.2279 Loss_G: 0.6639\n",
            "===> Epoch[166](15/23): Loss_D: 0.2296 Loss_G: 0.6331\n",
            "===> Epoch[166](20/23): Loss_D: 0.2482 Loss_G: 0.6074\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.1579 dB\n",
            "  [DEBUG] Train: time spent in epoch 166 is 348.60644936561584\n",
            "===> Epoch[167](5/23): Loss_D: 0.2225 Loss_G: 0.6605\n",
            "===> Epoch[167](10/23): Loss_D: 0.2186 Loss_G: 0.6090\n",
            "===> Epoch[167](15/23): Loss_D: 0.2244 Loss_G: 0.6569\n",
            "===> Epoch[167](20/23): Loss_D: 0.2266 Loss_G: 0.6099\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0999 dB\n",
            "  [DEBUG] Train: time spent in epoch 167 is 344.6856939792633\n",
            "===> Epoch[168](5/23): Loss_D: 0.2059 Loss_G: 0.6615\n",
            "===> Epoch[168](10/23): Loss_D: 0.2078 Loss_G: 0.6707\n",
            "===> Epoch[168](15/23): Loss_D: 0.2299 Loss_G: 0.6565\n",
            "===> Epoch[168](20/23): Loss_D: 0.2116 Loss_G: 0.6279\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0105 dB\n",
            "  [DEBUG] Train: time spent in epoch 168 is 327.5913372039795\n",
            "===> Epoch[169](5/23): Loss_D: 0.2367 Loss_G: 0.5908\n",
            "===> Epoch[169](10/23): Loss_D: 0.1984 Loss_G: 0.6178\n",
            "===> Epoch[169](15/23): Loss_D: 0.2574 Loss_G: 0.5825\n",
            "===> Epoch[169](20/23): Loss_D: 0.2118 Loss_G: 0.6215\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0039 dB\n",
            "  [DEBUG] Train: time spent in epoch 169 is 345.29094648361206\n",
            "===> Epoch[170](5/23): Loss_D: 0.2213 Loss_G: 0.6364\n",
            "===> Epoch[170](10/23): Loss_D: 0.2356 Loss_G: 0.6193\n",
            "===> Epoch[170](15/23): Loss_D: 0.2125 Loss_G: 0.6901\n",
            "===> Epoch[170](20/23): Loss_D: 0.2171 Loss_G: 0.6900\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.2969 dB\n",
            "  [DEBUG] Train: time spent in epoch 170 is 350.5648102760315\n",
            "===> Epoch[171](5/23): Loss_D: 0.2238 Loss_G: 0.6427\n",
            "===> Epoch[171](10/23): Loss_D: 0.2262 Loss_G: 0.6470\n",
            "===> Epoch[171](15/23): Loss_D: 0.2307 Loss_G: 0.6456\n",
            "===> Epoch[171](20/23): Loss_D: 0.2287 Loss_G: 0.6065\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0658 dB\n",
            "  [DEBUG] Train: time spent in epoch 171 is 348.5120687484741\n",
            "===> Epoch[172](5/23): Loss_D: 0.2232 Loss_G: 0.6655\n",
            "===> Epoch[172](10/23): Loss_D: 0.2316 Loss_G: 0.6217\n",
            "===> Epoch[172](15/23): Loss_D: 0.2342 Loss_G: 0.6196\n",
            "===> Epoch[172](20/23): Loss_D: 0.2234 Loss_G: 0.6820\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.1965 dB\n",
            "  [DEBUG] Train: time spent in epoch 172 is 343.19441509246826\n",
            "===> Epoch[173](5/23): Loss_D: 0.2134 Loss_G: 0.6489\n",
            "===> Epoch[173](10/23): Loss_D: 0.2292 Loss_G: 0.6169\n",
            "===> Epoch[173](15/23): Loss_D: 0.2103 Loss_G: 0.6313\n",
            "===> Epoch[173](20/23): Loss_D: 0.2286 Loss_G: 0.5970\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0545 dB\n",
            "  [DEBUG] Train: time spent in epoch 173 is 348.2221291065216\n",
            "===> Epoch[174](5/23): Loss_D: 0.2625 Loss_G: 0.5832\n",
            "===> Epoch[174](10/23): Loss_D: 0.2053 Loss_G: 0.6452\n",
            "===> Epoch[174](15/23): Loss_D: 0.2175 Loss_G: 0.6253\n",
            "===> Epoch[174](20/23): Loss_D: 0.2441 Loss_G: 0.6065\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 8.0505 dB\n",
            "  [DEBUG] Train: time spent in epoch 174 is 354.73154282569885\n",
            "===> Epoch[175](5/23): Loss_D: 0.2337 Loss_G: 0.5743\n",
            "===> Epoch[175](10/23): Loss_D: 0.2201 Loss_G: 0.6518\n",
            "===> Epoch[175](15/23): Loss_D: 0.2399 Loss_G: 0.6031\n",
            "===> Epoch[175](20/23): Loss_D: 0.2157 Loss_G: 0.6706\n",
            "learning rate = 0.0020000\n",
            "learning rate = 0.0020000\n",
            "===> Avg. PSNR: 7.9869 dB\n",
            "  [DEBUG] Train: time spent in epoch 175 is 341.47743701934814\n",
            "Checkpoint for epoch 175 saved\n",
            "Ending training as stop_after_checkpoint is set to True\n",
            "\n",
            "Training ended. It took 8649.769093751907 seconds\n",
            "Arguments used: ['--cuda', '--epoch_count=151', '--niter=250', '--niter_decay=250', '--lr=0.002', '--lamb=1', '--direction=a2b', '--batch_size=5', '--checkpoint_epochs=25', '--threads=0', '--debug=1']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlJsxzBKw9vl"
      },
      "source": [
        "### Training results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDHXoF-Fh-nf"
      },
      "source": [
        "#### Epochs 126-150 (batch size of 10)\r\n",
        "```\r\n",
        "===> Avg. PSNR: 8.0881 dB\r\n",
        "  [DEBUG] Train: time spent in epoch 149 is 356.96931982040405\r\n",
        "===> Epoch[150](3/12): Loss_D: 0.2416 Loss_G: 0.6463\r\n",
        "===> Epoch[150](6/12): Loss_D: 0.2202 Loss_G: 0.6578\r\n",
        "===> Epoch[150](9/12): Loss_D: 0.2300 Loss_G: 0.6615\r\n",
        "===> Epoch[150](12/12): Loss_D: 0.2092 Loss_G: 0.6119\r\n",
        "learning rate = 0.0020000\r\n",
        "learning rate = 0.0020000\r\n",
        "===> Avg. PSNR: 7.9952 dB\r\n",
        "  [DEBUG] Train: time spent in epoch 150 is 363.01134037971497\r\n",
        "Checkpoint for epoch 150 saved\r\n",
        "Ending training as stop_after_checkpoint is set to True\r\n",
        "\r\n",
        "Training ended. It took 8968.985772371292 seconds\r\n",
        "Arguments used: ['--cuda', '--epoch_count=126', '--niter=250', '--niter_decay=250', '--lr=0.002', '--lamb=1', '--direction=a2b', '--batch_size=10', '--checkpoint_epochs=25', '--threads=0', '--debug=1']\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDe78p_YxKAT"
      },
      "source": [
        "#### Epochs 151-175 (batch size of 5)\r\n",
        "```\r\n",
        "===> Avg. PSNR: 8.0505 dB\r\n",
        "  [DEBUG] Train: time spent in epoch 174 is 354.73154282569885\r\n",
        "===> Epoch[175](5/23): Loss_D: 0.2337 Loss_G: 0.5743\r\n",
        "===> Epoch[175](10/23): Loss_D: 0.2201 Loss_G: 0.6518\r\n",
        "===> Epoch[175](15/23): Loss_D: 0.2399 Loss_G: 0.6031\r\n",
        "===> Epoch[175](20/23): Loss_D: 0.2157 Loss_G: 0.6706\r\n",
        "learning rate = 0.0020000\r\n",
        "learning rate = 0.0020000\r\n",
        "===> Avg. PSNR: 7.9869 dB\r\n",
        "  [DEBUG] Train: time spent in epoch 175 is 341.47743701934814\r\n",
        "Checkpoint for epoch 175 saved\r\n",
        "Ending training as stop_after_checkpoint is set to True\r\n",
        "\r\n",
        "Training ended. It took 8649.769093751907 seconds\r\n",
        "Arguments used: ['--cuda', '--epoch_count=151', '--niter=250', '--niter_decay=250', '--lr=0.002', '--lamb=1', '--direction=a2b', '--batch_size=5', '--checkpoint_epochs=25', '--threads=0', '--debug=1']\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhw6VB9yECIn"
      },
      "source": [
        "## Launch TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDN-_iThD4CH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "a4d7c108-4c77-4125-e7be-721de79d73a7"
      },
      "source": [
        "# Load extension\r\n",
        "%reload_ext tensorboard\r\n",
        "# Run TensorBoard on training directory: takes some seconds\r\n",
        "# It doesn't work with python variables: %tensorboard --logdir dir_log_train  <-- fails\r\n",
        "%tensorboard --logdir \"dataset/train/log\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}