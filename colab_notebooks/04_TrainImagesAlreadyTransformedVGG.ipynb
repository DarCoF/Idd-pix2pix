{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03.5-TrainImagesAlreadyTransformedVGG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dnj7r6_2_Hmi",
        "pgmec5oC_2ps",
        "Qmyzj3UIA3FS",
        "KKE6KGPy-I7z",
        "M_tQtrTvD_9v",
        "eadWfzY8EH47",
        "e-s3i7BqERIS",
        "LkGcPIL1EbNh",
        "WIcDgte_EiYR",
        "P0Zx0lQCEnVv",
        "LRql8ZFgEtwr",
        "2G3-634DE1Na",
        "qTaAiirPFA1L",
        "Cf5OAy96FQkq",
        "JNeXjgOPFWj2",
        "kqQHEbYLFi9D",
        "ZJEbSaM4Fu3b",
        "tMk0O6ZWF6fN",
        "fps3rv0eGA5Z",
        "VRQHgNJVSaOv",
        "dmhnfzo8p2wf",
        "N7gO9YJxwIjV",
        "pSHF47QatVcN"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N1-zk0B8JEK"
      },
      "source": [
        "# Train with VGG loss\r\n",
        "\r\n",
        "Colab with a pix2pix implementation based on the code found in https://github.com/mrzhu-cool/pix2pix-pytorch and adapted to use the VGG loss as a content loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EyyleZS8ZLh"
      },
      "source": [
        "## Imports and parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnj7r6_2_Hmi"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSkR6PpC8czK"
      },
      "source": [
        "# Accessing the files and preparing the dataset\r\n",
        "from google.colab import drive\r\n",
        "from os import listdir\r\n",
        "from os.path import join\r\n",
        "import os\r\n",
        "\r\n",
        "# Treating the images\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import torch\r\n",
        "import torch.utils.data as data\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from matplotlib.pyplot import imshow\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Dealing with GPUs\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "\r\n",
        "# Defining the networks\r\n",
        "import torch.nn as nn\r\n",
        "from torch.nn import init\r\n",
        "import functools\r\n",
        "from torch.optim import lr_scheduler\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "# Training\r\n",
        "from math import log10\r\n",
        "import time\r\n",
        "import math\r\n",
        "\r\n",
        "# Tensorboard\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBzHI4aC_Jxo"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDGdBS9o8iOC"
      },
      "source": [
        "import argparse\r\n",
        "\r\n",
        "# Training settings\r\n",
        "parser = argparse.ArgumentParser(description='pix2pix-pytorch-implementation')\r\n",
        "# In the original code, dataset is required. We don't need it for the Inria Aerial Image Labelling Dataset\r\n",
        "parser.add_argument('--dataset', required=False, help='facades')\r\n",
        "parser.add_argument('--batch_size', type=int, default=1, help='training batch size')\r\n",
        "parser.add_argument('--test_batch_size', type=int, default=1, help='testing batch size')\r\n",
        "parser.add_argument('--direction', type=str, default='a2b', help='a2b or b2a')\r\n",
        "parser.add_argument('--input_nc', type=int, default=3, help='input image channels')\r\n",
        "parser.add_argument('--output_nc', type=int, default=3, help='output image channels')\r\n",
        "parser.add_argument('--ngf', type=int, default=64, help='generator filters in first conv layer')\r\n",
        "parser.add_argument('--ndf', type=int, default=64, help='discriminator filters in first conv layer')\r\n",
        "# Training epochs are defined by range(opt.epoch_count, opt.niter + opt.niter_decay + 1)\r\n",
        "# So, originally, the training script epochs from 1 to 201, which takes too long at the beginning\r\n",
        "# niter and niter_decay are changed to shorten the amount of time during development\r\n",
        "parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count')\r\n",
        "parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate') # 100\r\n",
        "parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero') # 100\r\n",
        "parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam') # 0.0002\r\n",
        "parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau|cosine')\r\n",
        "parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\r\n",
        "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\r\n",
        "parser.add_argument('--cuda', action='store_true', help='use cuda?')\r\n",
        "parser.add_argument('--threads', type=int, default=1, help='number of threads for data loader to use')\r\n",
        "parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\r\n",
        "parser.add_argument('--lamb', type=int, default=10, help='weight on L1 term in objective')\r\n",
        "# lamb2  VGG loss\r\n",
        "parser.add_argument('--lamb2', type=int, default=10, help='weight on VGG term in objective')\r\n",
        "# Size of the side of the image. Only square images are accepted\r\n",
        "parser.add_argument('--input_image_size', type=int, default=286, help='input image side size')\r\n",
        "# Size of the side of the image to be fed to the model. It will generate a squared image\r\n",
        "parser.add_argument('--model_side_size', type=int, default=256, help='image fed to the model side size')\r\n",
        "# Activate or deactivate the use of Tensorboard\r\n",
        "parser.add_argument('--tb_active', type=bool, default=True, help='should tensorboard be used') # Deactivate for deep trainings\r\n",
        "# Which original image should be stored in Tensorboard.\r\n",
        "# Inria satellite images are 5000x5000 and consume much CPU and memory, so only\r\n",
        "# one image is saved to avoid using too many resources\r\n",
        "parser.add_argument('--tb_image', type=str, default='vienna1.npy', help='image to store in tensorboard')\r\n",
        "# Number of images saved to tensorboard. Only tb_image will be saved, so the progress\r\n",
        "# of generated images can be seen throw epochs. 5 images in 100 epochs means one\r\n",
        "# tb_image will be saved every 20 epochs.\r\n",
        "parser.add_argument('--tb_number_img', type=int, default=5, help='number of images saved to tensorboard')\r\n",
        "# Level of debug (cell output)\r\n",
        "parser.add_argument('--debug', type=int, default=0, help='level of debug from 0 (no debug) to 2 (verbose)')\r\n",
        "# Number of iteration messages per epoch. They have the form\r\n",
        "# ===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\r\n",
        "parser.add_argument('--iter_messages', type=int, default=4, help='number of output messages per epoch')\r\n",
        "# Number of epochs to save a checkpoint\r\n",
        "parser.add_argument('--checkpoint_epochs', type=int, default=50, help='number of epochs to save a checkpoint')\r\n",
        "# Stop training after checkpoint is saved. Useful in long trainings\r\n",
        "parser.add_argument('--stop_after_checkpoint', type=int, default=1, help='stop training after a checkpoint has been saved')\r\n",
        "\r\n",
        "# As stated in https://stackoverflow.com/questions/48796169/how-to-fix-ipykernel-launcher-py-error-unrecognized-arguments-in-jupyter\r\n",
        "# at least an empty list must be passed to simulate a script execution with no parameters.\r\n",
        "# If no parameter is provided, parse_args tries to read _sys.argv[1:], which is not defined\r\n",
        "# in a colab execution\r\n",
        "training_args = ['--cuda',\r\n",
        "                 '--epoch_count=1',\r\n",
        "                 '--niter=450',\r\n",
        "                 '--niter_decay=450',\r\n",
        "                 '--lr=0.0002',\r\n",
        "                 '--lamb=100',\r\n",
        "                 '--lamb2=100',  #vgg\r\n",
        "                 '--direction=a2b',\r\n",
        "                 # A batch size of 11 raises an error:\r\n",
        "                 # RuntimeError: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 11.17 GiB total capacity; 7.90 GiB already allocated; 112.81 MiB free; 10.61 GiB reserved in total by PyTorch)\r\n",
        "                 '--batch_size=4',\r\n",
        "                 '--test_batch_size=2',\r\n",
        "                 '--input_image_size=286',\r\n",
        "                 '--model_side_size=256',\r\n",
        "                 '--checkpoint_epochs=150',\r\n",
        "                 '--stop_after_checkpoint=1',\r\n",
        "                 '--threads=0',\r\n",
        "                 '--debug=1',\r\n",
        "                 '--tb_number_img=9',\r\n",
        "                 '--iter_messages=2']\r\n",
        "opt = parser.parse_args(training_args)\r\n",
        "\r\n",
        "train_dir = 'dataset/train'\r\n",
        "train_gt_dir = train_dir + '/gt'\r\n",
        "train_images_dir = train_dir + '/images'\r\n",
        "# train_tensorboard_dir = train_dir + '/log'\r\n",
        "train_tensorboard_dir = '/content/drive/MyDrive/Colab Notebooks/AIDL/Project/nosplit542/log/1200epochs'\r\n",
        "\r\n",
        "test_dir = 'dataset/test'\r\n",
        "test_gt_dir = test_dir + '/gt'\r\n",
        "test_images_dir = test_dir + '/images'\r\n",
        "# test_tensorboard_dir = test_dir + '/log'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rFyZk4__PvK"
      },
      "source": [
        "if opt.cuda and not torch.cuda.is_available():\r\n",
        "    raise Exception(\"No GPU found, please run without --cuda\")\r\n",
        "\r\n",
        "cudnn.benchmark = True\r\n",
        "\r\n",
        "torch.manual_seed(opt.seed)\r\n",
        "if opt.cuda:\r\n",
        "    torch.cuda.manual_seed(opt.seed)\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgmec5oC_2ps"
      },
      "source": [
        "### Debug function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSno8WYC_4ir"
      },
      "source": [
        "def print_debug(level, text):\r\n",
        "    \"\"\"\r\n",
        "    Prints a debug message only if the level of the message is lower or equal\r\n",
        "    to the debug level set in global variable debug\r\n",
        "    \"\"\"\r\n",
        "    # Accessing the global debug variable\r\n",
        "    # global debug\r\n",
        "    # The text will only be\r\n",
        "    if level <= opt.debug:\r\n",
        "        print(\"  [DEBUG] \" + text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-QxR1np9gFi"
      },
      "source": [
        "## Accessing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHU6-BqR_4Ej"
      },
      "source": [
        "### Defining the dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7lfv2t09lDs"
      },
      "source": [
        "# from utils import is_image_file, load_img\r\n",
        "def is_image_file(filename):\r\n",
        "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".npy\"])\r\n",
        "\r\n",
        "class DatasetFromFolder(data.Dataset):\r\n",
        "    def __init__(self, image_dir, direction=\"a2b\"):\r\n",
        "        \"\"\"\r\n",
        "        Constructor adapted to the characteristics of the https://project.inria.fr/aerialimagelabeling/\r\n",
        "        images split as follows:\r\n",
        "        - train/a: training mask (ground truth) images\r\n",
        "        - train/b: training satellite images\r\n",
        "        - test/a: test mask (ground truth) images\r\n",
        "        - test/b: test satellite images\r\n",
        "\r\n",
        "        Example of use:\r\n",
        "        train_ds = DatasetFromFolder(\"/content/drive/MyDrive/Colab Notebooks/AIDL/Project/train\", \"a2b\")\r\n",
        "        \"\"\"\r\n",
        "        super(DatasetFromFolder, self).__init__()\r\n",
        "        self.direction = direction\r\n",
        "        self.a_path = join(image_dir, \"gt\")  # mask (ground truth) images. Originally \"a\"\r\n",
        "        self.b_path = join(image_dir, \"images\")  # satellite images. Originally \"b\"\r\n",
        "        self.image_filenames = [x for x in listdir(self.a_path) if is_image_file(x)]\r\n",
        "\r\n",
        "        transform_list = [transforms.ToTensor(),\r\n",
        "                          # Even if masks have only one channel, they're converted to RGB in __getitem__\r\n",
        "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\r\n",
        "        self.transform = transforms.Compose(transform_list)\r\n",
        "\r\n",
        "        # Test npy storage in memory. Does the data loader keep the instance of\r\n",
        "        # this dataset between epochs?\r\n",
        "        self.a_image_tensors = {}\r\n",
        "        self.b_image_tensors = {}\r\n",
        "        \r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        filename = self.image_filenames[index]\r\n",
        "        print_debug(2, \"DatasetFromFolder __getitem__: getting item {} corresponding to file {}\".format(index, filename))\r\n",
        "        \r\n",
        "        # Checking if file is already in memory\r\n",
        "        if filename in self.a_image_tensors.keys():\r\n",
        "            if filename == opt.tb_image:\r\n",
        "                print_debug(2, \"DatasetFromFolder __getitem__: {} found in memory.Total stored: {}\".format(filename, len(self.a_image_tensors.keys())))\r\n",
        "            # If it is already in memory, a copy is used\r\n",
        "            a = self.a_image_tensors[filename].clone()\r\n",
        "            b = self.b_image_tensors[filename].clone()\r\n",
        "        else:\r\n",
        "            # If the image is not in memory, it is read and transformed to a Tensor\r\n",
        "\r\n",
        "            # This Colab reads already pretransformed images. They were already converted to RGB\r\n",
        "            # a = Image.open(join(self.a_path, self.image_filenames[index])).convert('RGB')\r\n",
        "            # b = Image.open(join(self.b_path, self.image_filenames[index])).convert('RGB')\r\n",
        "            # a = Image.open(join(self.a_path, self.image_filenames[index]))\r\n",
        "            # b = Image.open(join(self.b_path, self.image_filenames[index]))\r\n",
        "            a = np.load(join(self.a_path, self.image_filenames[index]))\r\n",
        "            b = np.load(join(self.b_path, self.image_filenames[index]))\r\n",
        "\r\n",
        "            print_debug(2, \"a_{} [0,3,7]:{:.4f} [1,100,100]:{:.4f} [2,200,200]:{:.4f}\".format(\r\n",
        "              self.image_filenames[index],\r\n",
        "              a[3,7,0],\r\n",
        "              a[100,100,1],\r\n",
        "              a[200,200,2]\r\n",
        "            ))\r\n",
        "            print_debug(2, \"b_{} [0,3,7]:{:.4f} [1,100,100]:{:.4f} [2,200,200]:{:.4f}\".format(\r\n",
        "              self.image_filenames[index],\r\n",
        "              b[3,7,0],\r\n",
        "              b[100,100,1],\r\n",
        "              b[200,200,2]\r\n",
        "            ))\r\n",
        "\r\n",
        "            # Pretransformed images are already 286x286 size\r\n",
        "            # a = a.resize((286, 286), Image.BICUBIC) # Revision pending: from 5000x5000 to 286x286 sizes. This can lead to learning problems\r\n",
        "            # b = b.resize((286, 286), Image.BICUBIC)\r\n",
        "            a = transforms.ToTensor()(a)\r\n",
        "            b = transforms.ToTensor()(b)\r\n",
        "\r\n",
        "            # Storing tensors for future use. To avoid memory explosion, only a few images are stored\r\n",
        "            if filename == opt.tb_image or len(self.a_image_tensors.keys()) < 200:\r\n",
        "                if filename == opt.tb_image:\r\n",
        "                    print_debug(2, \"DatasetFromFolder __getitem__: storing {} for future reuse\".format(filename))\r\n",
        "                self.a_image_tensors[filename] = a.clone()\r\n",
        "                self.b_image_tensors[filename] = b.clone()\r\n",
        "\r\n",
        "        w_offset = random.randint(0, max(0, opt.input_image_size - opt.model_side_size - 1)) # \r\n",
        "        h_offset = random.randint(0, max(0, opt.input_image_size - opt.model_side_size - 1))\r\n",
        "    \r\n",
        "        a = a[:, h_offset:h_offset + opt.model_side_size, w_offset:w_offset + opt.model_side_size]\r\n",
        "        b = b[:, h_offset:h_offset + opt.model_side_size, w_offset:w_offset + opt.model_side_size]\r\n",
        "    \r\n",
        "        # Pretransformed images are already normalized\r\n",
        "        # a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\r\n",
        "        # b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\r\n",
        "\r\n",
        "        if random.random() < 0.5:\r\n",
        "            idx = [i for i in range(a.size(2) - 1, -1, -1)]\r\n",
        "            idx = torch.LongTensor(idx)\r\n",
        "            a = a.index_select(2, idx)\r\n",
        "            b = b.index_select(2, idx)\r\n",
        "\r\n",
        "        if self.direction == \"a2b\":\r\n",
        "            return a, b, filename\r\n",
        "        else:\r\n",
        "            return b, a, filename\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.image_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmyzj3UIA3FS"
      },
      "source": [
        "### Connecting to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXmOv_spA5Fp",
        "outputId": "3cd9bb6f-552d-4709-8299-677bf7096043"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfJxcVIjy0C_"
      },
      "source": [
        "### Copying files from Drive to CoLab machine\r\n",
        "\r\n",
        "According to this [article](https://enjoymachinelearning.com/posts/colab-with-google-drive/), reading files from the local storage in Google Colab is faster than doing so from Google Drive. Since all the training files are read once per epoch, it makes sense to copy them and work in the local filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc2i8PLfzADE"
      },
      "source": [
        "# Create directories where training and test images will be copied from Drive\r\n",
        "os.makedirs(name = train_gt_dir, exist_ok=True)\r\n",
        "os.makedirs(name = train_images_dir, exist_ok=True)\r\n",
        "# os.makedirs(name = train_tensorboard_dir, exist_ok=True)\r\n",
        "os.makedirs(name = test_gt_dir, exist_ok=True)\r\n",
        "os.makedirs(name = test_images_dir, exist_ok=True)\r\n",
        "# os.makedirs(name = test_tensorboard_dir, exist_ok=True)\r\n",
        "\r\n",
        "# Just in case, all files are deleted\r\n",
        "!rm \"{train_gt_dir}\"/*\r\n",
        "!rm \"{train_images_dir}\"/*\r\n",
        "!rm \"{test_gt_dir}\"/*\r\n",
        "!rm \"{test_images_dir}\"/*\r\n",
        "\r\n",
        "# Copy files ending in 0, 1, 2 or 3 to train directories. Shell commands don't accept python variables?\r\n",
        "!cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/nosplit/gt/*[0-6].npy \"{train_gt_dir}\"\r\n",
        "!cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/nosplit/images/*[0-6].npy \"{train_images_dir}\"\r\n",
        "# !cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/AerialImageDataset-pix2pix/gt/*11.npy \"{train_gt_dir}\"\r\n",
        "# !cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/AerialImageDataset-pix2pix/images/*11.npy \"{train_images_dir}\"\r\n",
        "\r\n",
        "# Copy files ending in 9 as test images\r\n",
        "!cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/nosplit/gt/*[7-8].npy \"{test_gt_dir}\"\r\n",
        "!cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/nosplit/images/*[7-8].npy \"{test_images_dir}\"\r\n",
        "# !cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/AerialImageDataset-pix2pix/gt/*28.npy \"{test_gt_dir}\"\r\n",
        "# !cp /content/drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/AerialImageDataset-pix2pix/images/*28.npy \"{test_images_dir}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r7dXnK3AScL"
      },
      "source": [
        "### Creating the data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A21ZTd7AWCx"
      },
      "source": [
        "train_set = DatasetFromFolder(train_dir, opt.direction) # a2b is \"gt\" to \"images\"\r\n",
        "test_set  = DatasetFromFolder(test_dir, opt.direction)  # b2a is \"images\" to \"gt\"\r\n",
        "training_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batch_size, shuffle=True)\r\n",
        "testing_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.test_batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKE6KGPy-I7z"
      },
      "source": [
        "## Defining the networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_tQtrTvD_9v"
      },
      "source": [
        "### get_norm_layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtH0aSdlEEeK"
      },
      "source": [
        "def get_norm_layer(norm_type='instance'):\r\n",
        "    if norm_type == 'batch':\r\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\r\n",
        "    elif norm_type == 'instance':\r\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\r\n",
        "    elif norm_type == 'switchable':\r\n",
        "        norm_layer = SwitchNorm2d\r\n",
        "    elif norm_type == 'none':\r\n",
        "        norm_layer = None\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\r\n",
        "    return norm_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eadWfzY8EH47"
      },
      "source": [
        "### get_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCCLVUCNELDi"
      },
      "source": [
        "def get_scheduler(optimizer, opt):\r\n",
        "    if opt.lr_policy == 'lambda':\r\n",
        "        def lambda_rule(epoch):\r\n",
        "            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\r\n",
        "            return lr_l\r\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\r\n",
        "    elif opt.lr_policy == 'step':\r\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\r\n",
        "    elif opt.lr_policy == 'plateau':\r\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\r\n",
        "    elif opt.lr_policy == 'cosine':\r\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\r\n",
        "    else:\r\n",
        "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\r\n",
        "    return scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-s3i7BqERIS"
      },
      "source": [
        "### update_learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zZuU8L-ETRU"
      },
      "source": [
        "def update_learning_rate(scheduler, optimizer, loss=None):\r\n",
        "    if opt.lr_policy == 'plateau':\r\n",
        "        scheduler.step(loss)\r\n",
        "    else:\r\n",
        "        scheduler.step()\r\n",
        "    lr = optimizer.param_groups[0]['lr']\r\n",
        "    print('learning rate = %.7f' % lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkGcPIL1EbNh"
      },
      "source": [
        "### init_weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIq4Rj0hEcfK"
      },
      "source": [
        "def init_weights(net, init_type='normal', gain=0.02):\r\n",
        "    def init_func(m):\r\n",
        "        classname = m.__class__.__name__\r\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\r\n",
        "            if init_type == 'normal':\r\n",
        "                init.normal_(m.weight.data, 0.0, gain)\r\n",
        "            elif init_type == 'xavier':\r\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\r\n",
        "            elif init_type == 'kaiming':\r\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\r\n",
        "            elif init_type == 'orthogonal':\r\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\r\n",
        "            else:\r\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\r\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\r\n",
        "                init.constant_(m.bias.data, 0.0)\r\n",
        "        elif classname.find('BatchNorm2d') != -1:\r\n",
        "            init.normal_(m.weight.data, 1.0, gain)\r\n",
        "            init.constant_(m.bias.data, 0.0)\r\n",
        "\r\n",
        "    print('initialize network with %s' % init_type)\r\n",
        "    net.apply(init_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIcDgte_EiYR"
      },
      "source": [
        "### init_net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRuLD2VaEkOC"
      },
      "source": [
        "def init_net(net, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\r\n",
        "    net.to(gpu_id)\r\n",
        "    init_weights(net, init_type, gain=init_gain)\r\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Zx0lQCEnVv"
      },
      "source": [
        "### define_G"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn_85WqnEpb5"
      },
      "source": [
        "def define_G(input_nc, output_nc, ngf, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\r\n",
        "    net = None\r\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\r\n",
        "\r\n",
        "    net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\r\n",
        "   \r\n",
        "    return init_net(net, init_type, init_gain, gpu_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRql8ZFgEtwr"
      },
      "source": [
        "### Class ResnetGenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a17K64xEwgC"
      },
      "source": [
        "# Defines the generator that consists of Resnet blocks between a few\r\n",
        "# downsampling/upsampling operations.\r\n",
        "class ResnetGenerator(nn.Module):\r\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=9, padding_type='reflect'):\r\n",
        "        assert(n_blocks >= 0)\r\n",
        "        super(ResnetGenerator, self).__init__()\r\n",
        "        self.input_nc = input_nc\r\n",
        "        self.output_nc = output_nc\r\n",
        "        self.ngf = ngf\r\n",
        "        if type(norm_layer) == functools.partial:\r\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\r\n",
        "        else:\r\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\r\n",
        "\r\n",
        "        self.inc = Inconv(input_nc, ngf, norm_layer, use_bias)\r\n",
        "        self.down1 = Down(ngf, ngf * 2, norm_layer, use_bias)\r\n",
        "        self.down2 = Down(ngf * 2, ngf * 4, norm_layer, use_bias)\r\n",
        "\r\n",
        "        model = []\r\n",
        "        for i in range(n_blocks):\r\n",
        "            model += [ResBlock(ngf * 4, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\r\n",
        "        self.resblocks = nn.Sequential(*model)\r\n",
        "\r\n",
        "        self.up1 = Up(ngf * 4, ngf * 2, norm_layer, use_bias)\r\n",
        "        self.up2 = Up(ngf * 2, ngf, norm_layer, use_bias)\r\n",
        "\r\n",
        "        self.outc = Outconv(ngf, output_nc)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        out = {}\r\n",
        "        # DTT No hay skip connections?\r\n",
        "        out['in'] = self.inc(input)\r\n",
        "        out['d1'] = self.down1(out['in'])\r\n",
        "        out['d2'] = self.down2(out['d1'])\r\n",
        "        out['bottle'] = self.resblocks(out['d2'])\r\n",
        "        out['u1'] = self.up1(out['bottle'])\r\n",
        "        out['u2'] = self.up2(out['u1'])\r\n",
        "\r\n",
        "        return self.outc(out['u2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G3-634DE1Na"
      },
      "source": [
        "### Class Inconv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek0MJpq5E3GL"
      },
      "source": [
        "class Inconv(nn.Module):\r\n",
        "    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\r\n",
        "        super(Inconv, self).__init__()\r\n",
        "        self.inconv = nn.Sequential(\r\n",
        "            nn.ReflectionPad2d(3),\r\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=7, padding=0,\r\n",
        "                      bias=use_bias),\r\n",
        "            norm_layer(out_ch),\r\n",
        "            nn.ReLU(True)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.inconv(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ReF-Na4E7Ca"
      },
      "source": [
        "Class Down"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tulCKo7E9Cr"
      },
      "source": [
        "class Down(nn.Module):\r\n",
        "    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\r\n",
        "        super(Down, self).__init__()\r\n",
        "        self.down = nn.Sequential(\r\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3,\r\n",
        "                      stride=2, padding=1, bias=use_bias),\r\n",
        "            norm_layer(out_ch),\r\n",
        "            nn.ReLU(True)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.down(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTaAiirPFA1L"
      },
      "source": [
        "### Class ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gzyti8rFC-6"
      },
      "source": [
        "# Define a Resnet block\r\n",
        "class ResBlock(nn.Module):\r\n",
        "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\r\n",
        "        super(ResBlock, self).__init__()\r\n",
        "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\r\n",
        "\r\n",
        "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\r\n",
        "        conv_block = []\r\n",
        "        p = 0\r\n",
        "        if padding_type == 'reflect':\r\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\r\n",
        "        elif padding_type == 'replicate':\r\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\r\n",
        "        elif padding_type == 'zero':\r\n",
        "            p = 1\r\n",
        "        else:\r\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\r\n",
        "\r\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\r\n",
        "                       norm_layer(dim),\r\n",
        "                       nn.ReLU(True)]\r\n",
        "        if use_dropout:\r\n",
        "            conv_block += [nn.Dropout(0.5)]\r\n",
        "\r\n",
        "        p = 0\r\n",
        "        if padding_type == 'reflect':\r\n",
        "            conv_block += [nn.ReflectionPad2d(1)]\r\n",
        "        elif padding_type == 'replicate':\r\n",
        "            conv_block += [nn.ReplicationPad2d(1)]\r\n",
        "        elif padding_type == 'zero':\r\n",
        "            p = 1\r\n",
        "        else:\r\n",
        "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\r\n",
        "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\r\n",
        "                       norm_layer(dim)]\r\n",
        "\r\n",
        "        return nn.Sequential(*conv_block)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # DTT 'x +' === skip connection!!\r\n",
        "        out = x + self.conv_block(x)\r\n",
        "        return nn.ReLU(True)(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf5OAy96FQkq"
      },
      "source": [
        "### Class Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCUVoDe5FSYt"
      },
      "source": [
        "class Up(nn.Module):\r\n",
        "    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\r\n",
        "        super(Up, self).__init__()\r\n",
        "        self.up = nn.Sequential(\r\n",
        "            # nn.Upsample(scale_factor=2, mode='nearest'),\r\n",
        "            # nn.Conv2d(in_ch, out_ch,\r\n",
        "            #           kernel_size=3, stride=1,\r\n",
        "            #           padding=1, bias=use_bias),\r\n",
        "            nn.ConvTranspose2d(in_ch, out_ch,\r\n",
        "                               kernel_size=3, stride=2,\r\n",
        "                               padding=1, output_padding=1,\r\n",
        "                               bias=use_bias),\r\n",
        "            norm_layer(out_ch),\r\n",
        "            nn.ReLU(True)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.up(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNeXjgOPFWj2"
      },
      "source": [
        "### Class Outconv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtKFFDlhFclO"
      },
      "source": [
        "class Outconv(nn.Module):\r\n",
        "    def __init__(self, in_ch, out_ch):\r\n",
        "        super(Outconv, self).__init__()\r\n",
        "        self.outconv = nn.Sequential(\r\n",
        "            nn.ReflectionPad2d(3),\r\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=7, padding=0),\r\n",
        "            nn.Tanh()\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.outconv(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqQHEbYLFi9D"
      },
      "source": [
        "### define_D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wade_AR1Fk6W"
      },
      "source": [
        "def define_D(input_nc, ndf, netD,\r\n",
        "             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\r\n",
        "    net = None\r\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\r\n",
        "\r\n",
        "    if netD == 'basic':\r\n",
        "        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\r\n",
        "    elif netD == 'n_layers':\r\n",
        "        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\r\n",
        "    elif netD == 'pixel':\r\n",
        "        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\r\n",
        "    else:\r\n",
        "        raise NotImplementedError('Discriminator model name [%s] is not recognized' % net)\r\n",
        "\r\n",
        "    return init_net(net, init_type, init_gain, gpu_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJEbSaM4Fu3b"
      },
      "source": [
        "### Class NLayerDiscriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13bSCt9BFz2m"
      },
      "source": [
        "# Defines the PatchGAN discriminator with the specified arguments.\r\n",
        "class NLayerDiscriminator(nn.Module):\r\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\r\n",
        "        super(NLayerDiscriminator, self).__init__()\r\n",
        "        if type(norm_layer) == functools.partial:\r\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\r\n",
        "        else:\r\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\r\n",
        "\r\n",
        "        kw = 4\r\n",
        "        padw = 1\r\n",
        "        sequence = [\r\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\r\n",
        "            nn.LeakyReLU(0.2, True)\r\n",
        "        ]\r\n",
        "\r\n",
        "        nf_mult = 1\r\n",
        "        nf_mult_prev = 1\r\n",
        "        for n in range(1, n_layers):\r\n",
        "            nf_mult_prev = nf_mult\r\n",
        "            nf_mult = min(2**n, 8)\r\n",
        "            sequence += [\r\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\r\n",
        "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\r\n",
        "                norm_layer(ndf * nf_mult),\r\n",
        "                nn.LeakyReLU(0.2, True)\r\n",
        "            ]\r\n",
        "\r\n",
        "        nf_mult_prev = nf_mult\r\n",
        "        nf_mult = min(2**n_layers, 8)\r\n",
        "        sequence += [\r\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\r\n",
        "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\r\n",
        "            norm_layer(ndf * nf_mult),\r\n",
        "            nn.LeakyReLU(0.2, True)\r\n",
        "        ]\r\n",
        "\r\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\r\n",
        "\r\n",
        "        if use_sigmoid:\r\n",
        "            sequence += [nn.Sigmoid()]\r\n",
        "\r\n",
        "        self.model = nn.Sequential(*sequence)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return self.model(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMk0O6ZWF6fN"
      },
      "source": [
        "### Class PixelDiscriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dniCAgHxF8pK"
      },
      "source": [
        "class PixelDiscriminator(nn.Module):\r\n",
        "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\r\n",
        "        super(PixelDiscriminator, self).__init__()\r\n",
        "        if type(norm_layer) == functools.partial:\r\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\r\n",
        "        else:\r\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\r\n",
        "\r\n",
        "        self.net = [\r\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\r\n",
        "            norm_layer(ndf * 2),\r\n",
        "            nn.LeakyReLU(0.2, True),\r\n",
        "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\r\n",
        "\r\n",
        "        if use_sigmoid:\r\n",
        "            self.net.append(nn.Sigmoid())\r\n",
        "\r\n",
        "        self.net = nn.Sequential(*self.net)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return self.net(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fps3rv0eGA5Z"
      },
      "source": [
        "### Class GANLoss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm8N57rx-K4p"
      },
      "source": [
        "class GANLoss(nn.Module):\r\n",
        "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\r\n",
        "        super(GANLoss, self).__init__()\r\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\r\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\r\n",
        "        if use_lsgan:\r\n",
        "            self.loss = nn.MSELoss()\r\n",
        "        else:\r\n",
        "            self.loss = nn.BCELoss()\r\n",
        "\r\n",
        "    def get_target_tensor(self, input, target_is_real):\r\n",
        "        if target_is_real:\r\n",
        "            target_tensor = self.real_label\r\n",
        "        else:\r\n",
        "            target_tensor = self.fake_label\r\n",
        "        return target_tensor.expand_as(input)\r\n",
        "\r\n",
        "    def __call__(self, input, target_is_real):\r\n",
        "        target_tensor = self.get_target_tensor(input, target_is_real)\r\n",
        "        return self.loss(input, target_tensor)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNiAHIgEBNdp"
      },
      "source": [
        "# VGG    \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nphAczC2BXbg"
      },
      "source": [
        "vgg_net = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)\r\n",
        " \r\n",
        "# Only the convolutional part is considered\r\n",
        "features = vgg_net.features\r\n",
        "# The last maxpool layer is removed\r\n",
        "features = nn.Sequential(*list(features.children())[:-1])\r\n",
        "features = features.to(device)\r\n",
        "\r\n",
        "criterionvgg = nn.L1Loss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7kQa3wkOrsk"
      },
      "source": [
        "## Auxiliary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRQHgNJVSaOv"
      },
      "source": [
        "### denormalize_image & show_image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjmrRT0ESdyO"
      },
      "source": [
        "def denormalize_image(image_tensor):\r\n",
        "    \"\"\"\r\n",
        "    Denormalizes an image coming from the network, usually, a generated image\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    images_tensor: tensor representing a PIL image\r\n",
        "    \"\"\"\r\n",
        "    print_debug(2, \"denormalize_image image tensor shape: {}\".format(image_tensor.shape))\r\n",
        "    # cpu() to avoid error \"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\"\r\n",
        "    image_numpy = image_tensor.cpu().data.float().numpy()\r\n",
        "    \r\n",
        "            # La transformación inversa sería simplemente min( (x*0.5)+0.5), 1)\r\n",
        "            # (haciendo un clipping de los valores para que no nos salgan colores raros).\r\n",
        "            # Tensorboard creo que ya gestiona lo del clipping;\r\n",
        "            # pero viene de nuestra cuenta hacer la \"desnormalización\".\r\n",
        "\r\n",
        "    print_debug(2, \"denormalize_image image_numpy shape: {}\".format(image_numpy.shape))\r\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\r\n",
        "    print_debug(2, \"denormalize_image image_numpy shape: {} after transposing\".format(image_numpy.shape))\r\n",
        "    image_numpy = image_numpy.clip(0, 255)\r\n",
        "    print_debug(2, \"denormalize_image image_numpy shape: {} after clipping\".format(image_numpy.shape))\r\n",
        "    image_numpy = image_numpy.astype(np.uint8)\r\n",
        "    print_debug(2, \"denormalize_image image_numpy shape: {} after converting to uint8\".format(image_numpy.shape))\r\n",
        "\r\n",
        "    return image_numpy\r\n",
        "\r\n",
        "def show_image(image_tensor):\r\n",
        "    \"\"\"\r\n",
        "    Shows an image coming from the network\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    \"\"\"\r\n",
        "    image_numpy = denormalize_image(image_tensor)\r\n",
        "    pil_image = Image.fromarray(image_numpy)\r\n",
        "    imshow(pil_image)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmhnfzo8p2wf"
      },
      "source": [
        "### Show list/tuple of images in a grid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx7fIkGUp9j6"
      },
      "source": [
        "# Based on utils.py save_img and the last answer in\r\n",
        "# https://stackoverflow.com/questions/46615554/how-to-display-multiple-images-in-one-figure-correctly/46616645#46616645\r\n",
        "# Plots several figures in a tile\r\n",
        "def show_images_grid(images_tuple, nrows=1, ncols=1):\r\n",
        "    \"\"\"\r\n",
        "    Shows several images coming from a DataLoader based on DatasetFromFolder\r\n",
        "    in a tile\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    images_tuple: tuple of tensors representing images\r\n",
        "    ncols : number of columns of subplots wanted in the display\r\n",
        "    nrows : number of rows of subplots wanted in the figure\r\n",
        "    \"\"\"\r\n",
        "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows, figsize=(15,15))\r\n",
        "    for ind,image_tensor in zip(range(len(images_tuple)), images_tuple):\r\n",
        "        # First, denormalize image to allow it to be printable\r\n",
        "        image_numpy = denormalize_image(image_tensor)\r\n",
        "        image_pil = Image.fromarray(image_numpy)\r\n",
        "        # imshow(image_pil)\r\n",
        "        \r\n",
        "        axeslist.ravel()[ind].imshow(image_pil, cmap=plt.jet())\r\n",
        "        # axeslist.ravel()[ind].set_title(title)\r\n",
        "        axeslist.ravel()[ind].set_axis_off()\r\n",
        "    plt.tight_layout() # optional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7gO9YJxwIjV"
      },
      "source": [
        "### setup_tensorboard_writer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p8NPbl6wRn8"
      },
      "source": [
        "def setup_tensorboard_writer(tensorboard_dir, model=None):\r\n",
        "    \"\"\"\r\n",
        "    Creates a new directory in tensorboard_dir to log data for TensorBoard.\r\n",
        "    If a model/net is provided, it is added to the writer.\r\n",
        "\r\n",
        "    Returns a reference to the writer\r\n",
        "    \"\"\"\r\n",
        "    # Setting up TensorBoard writer\r\n",
        "    # Creates a new directory to store TensorBoard data\r\n",
        "    log_subdir = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "    writer = SummaryWriter(log_dir=tensorboard_dir + \"/\" + log_subdir)\r\n",
        "\r\n",
        "    if model is not None:\r\n",
        "        # Adding the model to TensorBoard\r\n",
        "        # Apparently, TensorBoard only accepts one model per writer\r\n",
        "        writer.add_graph(model, input_to_model=torch.randn([1,3,256,256]).to(device))\r\n",
        "\r\n",
        "    return writer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uG68lfo0HZf"
      },
      "source": [
        "### save_iteration_tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWemRuem0OR1"
      },
      "source": [
        "def save_iteration_tensorboard(writer, epoch, iteration, loss_d, loss_g, loss_g_gan, loss_g_l1, loss_g_vgg\r\n",
        "                               real_a, real_b, fake_b, batch):\r\n",
        "    # VGG añadimos loss_g_vgg en los parametros de entrada de save_iteration_tensorboard \r\n",
        "    tensorboard_step = (len(training_data_loader.dataset.image_filenames) // opt.batch_size) * epoch + iteration\r\n",
        "    writer.add_scalar('Loss/D', loss_d.item(), global_step=tensorboard_step)\r\n",
        "    writer.add_scalar('Loss/G', loss_g.item(), global_step=tensorboard_step)\r\n",
        "    writer.add_scalar('Loss/G GAN', loss_g_gan.data, global_step=tensorboard_step)\r\n",
        "    writer.add_scalar('Loss/G L1', loss_g_l1.data, global_step=tensorboard_step)\r\n",
        "    writer.add_scalar('Loss/G VGG', loss_g_vgg.data, global_step=tensorboard_step)  # VGG\r\n",
        "\r\n",
        "    # DTT Decide whether saving images to tensorboard\r\n",
        "    final_epoch = opt.niter + opt.niter_decay + 1\r\n",
        "                                # final_epoch / opt.tb_number_img gives the number of epochs\r\n",
        "                                # that should pass before an image is saved.\r\n",
        "    epochs_to_pass = max(1, final_epoch // opt.tb_number_img) # at least should be 1\r\n",
        "    save_image_to_tensorboard = ( ( (epoch % epochs_to_pass == 0)\r\n",
        "                                # or it is the last epoch of training\r\n",
        "                                    or (epoch == final_epoch)\r\n",
        "                                  )\r\n",
        "                                  # it only saves the image if it corresponds to the defined opt.tb_image\r\n",
        "                                  and opt.tb_image in batch[2]\r\n",
        "    )\r\n",
        "    if save_image_to_tensorboard:\r\n",
        "        print_debug(2, \"save_iteration_tensorboard: saving {} to TensorBoard. Is in? {}. Batch: {}\".format(opt.tb_image, opt.tb_image in batch[2], batch[2]))\r\n",
        "        \r\n",
        "        batch_index = batch[2].index(opt.tb_image)\r\n",
        "        # DTT Write images to TensorBoard at the end of each epoch\r\n",
        "        writer.add_image(str(epoch)+'/1 Mask', real_a[batch_index], epoch)\r\n",
        "        # writer.add_image(str(epoch)+'/2 Normalized satellite image', real_b[batch_index], epoch)\r\n",
        "        # writer.add_image(str(epoch)+'/3 Generated satellite image', fake_b[batch_index], epoch)\r\n",
        "        writer.add_image(str(epoch)+'/4 Denormalized generated satellite image', denormalize_image(fake_b[batch_index]), epoch, dataformats='HWC')\r\n",
        "        # writer.add_image(str(epoch)+'/5 Original satellite image', batch[3][batch_index].squeeze(dim=0), epoch)\r\n",
        "    elif opt.tb_image in batch[2]:\r\n",
        "        print_debug(2, \"save_iteration_tensorboard: {} in batch, but won't save it. epochs_to_pass: {}. Batch: {}\".format(\r\n",
        "            opt.tb_image, epochs_to_pass, batch[2]))\r\n",
        "    else:\r\n",
        "        print_debug(2, \"save_iteration_tensorboard: won't save any image ({})\".format(batch[2]))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSHF47QatVcN"
      },
      "source": [
        "### save_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ROmgmG1tXmw"
      },
      "source": [
        "def save_checkpoint(epoch, net_g, net_d, optimizer_g, optimizer_d):\r\n",
        "    \"\"\"\r\n",
        "    Saves the discriminator and generator.\r\n",
        "    It returns a boolean stating whether training should stop or not\r\n",
        "    \"\"\"\r\n",
        "    if epoch % opt.checkpoint_epochs == 0:\r\n",
        "        checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/AIDL/Project/nosplit/checkpoint'\r\n",
        "        os.makedirs(name = checkpoint_dir, exist_ok=True)\r\n",
        "        net_g_model_out_path = os.path.join(checkpoint_dir, \"netG_model_epoch_{}.pth\".format(epoch))\r\n",
        "        net_d_model_out_path = os.path.join(checkpoint_dir, \"netD_model_epoch_{}.pth\".format(epoch))\r\n",
        "        optimizer_g_out_path = os.path.join(checkpoint_dir, \"adam_g_epoch_{}.pth\".format(epoch))\r\n",
        "        optimizer_d_out_path = os.path.join(checkpoint_dir, \"adam_d_epoch_{}.pth\".format(epoch))\r\n",
        "        # Baseline LR0.0002 lambda 100 with batch norm was saved like this\r\n",
        "        # torch.save(net_g, net_g_model_out_path)\r\n",
        "        # torch.save(net_d, net_d_model_out_path)\r\n",
        "        # torch.save(optimizer_g, optimizer_g_out_path)\r\n",
        "        # torch.save(optimizer_d, optimizer_d_out_path)\r\n",
        "\r\n",
        "        # Now saving only state dictionaries\r\n",
        "        torch.save({'net_g': net_g.state_dict(),\r\n",
        "            'optim_g': optimizer_g.state_dict()},\r\n",
        "            net_g_model_out_path)\r\n",
        "        torch.save({'net_d': net_d.state_dict(),\r\n",
        "                    'optim_d': optimizer_d.state_dict()},\r\n",
        "                   net_d_model_out_path)\r\n",
        "\r\n",
        "        print(\"Checkpoint for epoch {} saved\".format(epoch))\r\n",
        "\r\n",
        "        if opt.stop_after_checkpoint == 1:\r\n",
        "            return True\r\n",
        "\r\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6HwOMTGYQtX"
      },
      "source": [
        "## Creating/loading the networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO6RpCjdYUgD"
      },
      "source": [
        "# Creating the networks from scratch\r\n",
        "# Baseline LR 0.0002 Lambda 100 model was created with batch normalization.\r\n",
        "# net_g = define_G(opt.input_nc, opt.output_nc, opt.ngf, 'batch', False, 'normal', 0.02, gpu_id=device)\r\n",
        "# Let's try with instance normalization\r\n",
        "net_g = define_G(opt.input_nc, opt.output_nc, opt.ngf, 'instance', False, 'normal', 0.02, gpu_id=device)\r\n",
        "net_d = define_D(opt.input_nc + opt.output_nc, opt.ndf, 'basic', gpu_id=device)\r\n",
        "\r\n",
        "criterionGAN = GANLoss().to(device)\r\n",
        "criterionL1 = nn.L1Loss().to(device)\r\n",
        "criterionMSE = nn.MSELoss().to(device)\r\n",
        "\r\n",
        "# setup optimizer\r\n",
        "optimizer_g = optim.Adam(net_g.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\r\n",
        "optimizer_d = optim.Adam(net_d.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\r\n",
        "net_g_scheduler = get_scheduler(optimizer_g, opt)\r\n",
        "net_d_scheduler = get_scheduler(optimizer_d, opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV5f5MC3vrCm"
      },
      "source": [
        "## Loading previous trained checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fhR6isyblXF"
      },
      "source": [
        "# !ls drive/MyDrive/\"Colab Notebooks\"/AIDL/Project/train/log/net*.pth\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEDth9GOWXQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762c6a6d-75c2-4e94-e1ba-50ca34e7910a"
      },
      "source": [
        "criterionGAN = GANLoss().to(device)\r\n",
        "criterionL1 = nn.L1Loss().to(device)\r\n",
        "criterionMSE = nn.MSELoss().to(device)\r\n",
        "\r\n",
        "# Creating the networks from scratch\r\n",
        "# net_g = define_G(opt.input_nc, opt.output_nc, opt.ngf, 'batch', False, 'normal', 0.02, gpu_id=device)\r\n",
        "# net_d = define_D(opt.input_nc + opt.output_nc, opt.ndf, 'basic', gpu_id=device)\r\n",
        "\r\n",
        "# Loading already calculated weights\r\n",
        "checkpoints_path = 'drive/MyDrive/Colab Notebooks/AIDL/Project/nosplit/checkpoint'\r\n",
        "checkpoints_epoch = '750'\r\n",
        "\r\n",
        "# If no save_state_dict was used\r\n",
        "# net_g = torch.load(join(checkpoints_path, 'netG_model_epoch_{}.pth'.format(checkpoints_epoch)), map_location=torch.device(device)).to(device)\r\n",
        "# net_d = torch.load(join(checkpoints_path, 'netD_model_epoch_{}.pth'.format(checkpoints_epoch)), map_location=torch.device(device)).to(device)\r\n",
        "\r\n",
        "# If save_state_dict was used\r\n",
        "net_g = define_G(opt.input_nc, opt.output_nc, opt.ngf, 'instance', False, 'normal', 0.02, gpu_id=device)\r\n",
        "net_d = define_D(opt.input_nc + opt.output_nc, opt.ndf, 'basic', gpu_id=device)\r\n",
        "\r\n",
        "checkpoint_g = torch.load(join(checkpoints_path, 'netG_model_epoch_{}.pth'.format(checkpoints_epoch)), map_location=torch.device(device))\r\n",
        "net_g.load_state_dict(checkpoint_g['net_g'])\r\n",
        "\r\n",
        "checkpoint_d = torch.load(join(checkpoints_path, 'netD_model_epoch_{}.pth'.format(checkpoints_epoch)), map_location=torch.device(device))\r\n",
        "net_d.load_state_dict(checkpoint_d['net_d'])\r\n",
        "\r\n",
        "# setup optimizer. In this case, even if we start from a pretrained model, we will continue training with no gradients values\r\n",
        "optimizer_g = optim.Adam(net_g.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\r\n",
        "optimizer_d = optim.Adam(net_d.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\r\n",
        "\r\n",
        "optimizer_g.load_state_dict(checkpoint_g['optim_g'])\r\n",
        "optimizer_d.load_state_dict(checkpoint_d['optim_d'])\r\n",
        "\r\n",
        "net_g_scheduler = get_scheduler(optimizer_g, opt)\r\n",
        "net_d_scheduler = get_scheduler(optimizer_d, opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialize network with normal\n",
            "initialize network with normal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0xPUFQ6SDVt"
      },
      "source": [
        "# net_d = torch.load('checkpoint/netD_model_epoch_50.pth', map_location=torch.device(device)).to(device)\r\n",
        "# net_g = torch.load('checkpoint/netG_model_epoch_50.pth', map_location=torch.device(device)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlxLU_Uj-Tkx"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmpY-pNqHhNd"
      },
      "source": [
        "if opt.tb_active:\r\n",
        "    writer_train = setup_tensorboard_writer(train_tensorboard_dir, model=net_g)\r\n",
        "\r\n",
        "output_images = []\r\n",
        "start_time = time.time()\r\n",
        "\r\n",
        "# Training function\r\n",
        "for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\r\n",
        "    epoch_start_time = time.time()\r\n",
        "    # train\r\n",
        "    for iteration, batch in enumerate(training_data_loader, 1):\r\n",
        "        # forward\r\n",
        "        # DTT a: masks, b: satellite image\r\n",
        "        real_a, real_b = batch[0].to(device), batch[1].to(device)\r\n",
        "        # DTT fake_b: generated satellite image from generator\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** self.fake_B = self.netG(self.real_A)  # G(A)\r\n",
        "        fake_b = net_g(real_a)\r\n",
        "\r\n",
        "        ######################\r\n",
        "        # (1) Update D network\r\n",
        "        ######################\r\n",
        "\r\n",
        "        optimizer_d.zero_grad()\r\n",
        "        \r\n",
        "        # train with fake\r\n",
        "        # DTT Concatenates the real mask with generated image\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\r\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\r\n",
        "\r\n",
        "        # DTT Discriminator's prediction stating if the couple of images are\r\n",
        "        #     (real, real) o (real, false)\r\n",
        "        #     detach() to avoid calculating gradients\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** pred_fake = self.netD(fake_AB.detach())\r\n",
        "        #     ** # Fake; stop backprop to the generator by detaching fake_B\r\n",
        "        pred_fake = net_d.forward(fake_ab.detach())\r\n",
        "\r\n",
        "        # DTT Calculated losses where extremely big. Debug message to see why\r\n",
        "        print_debug(2, \"Train: pred_fake's shape {}, min {} and max {}\".format(\r\n",
        "            pred_fake.shape, pred_fake.min(), pred_fake.max()\r\n",
        "        ))\r\n",
        "\r\n",
        "        # DTT Loss when a generated image is fed. Should classificate it as False\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** self.loss_D_fake = self.criterionGAN(pred_fake, False)\r\n",
        "        loss_d_fake = criterionGAN(pred_fake, False)\r\n",
        "\r\n",
        "        # train with real\r\n",
        "        # DTT Concatenates the same real mask with its corresponding real image\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** real_AB = torch.cat((self.real_A, self.real_B), 1)\r\n",
        "        real_ab = torch.cat((real_a, real_b), 1)\r\n",
        "        # DTT Discriminator's prediction. Now calculating gradients\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** pred_real = self.netD(real_AB)\r\n",
        "        pred_real = net_d.forward(real_ab)\r\n",
        "        # DTT Discriminator should predict True with a real mask + image couple\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** self.loss_D_real = self.criterionGAN(pred_real, True)\r\n",
        "        loss_d_real = criterionGAN(pred_real, True)\r\n",
        "        \r\n",
        "        # Combined D loss\r\n",
        "        # DTT D's loss is the mean between its capacity ot detect a generated image\r\n",
        "        #     and its capacity to detect a real image\r\n",
        "        #     ** Code from original pix2pix implementation:\r\n",
        "        #     ** # combine loss and calculate gradients\r\n",
        "        #     ** self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\r\n",
        "        loss_d = (loss_d_fake + loss_d_real) * 0.5\r\n",
        "\r\n",
        "        loss_d.backward()\r\n",
        "       \r\n",
        "        optimizer_d.step()\r\n",
        "\r\n",
        "        ######################\r\n",
        "        # (2) Update G network\r\n",
        "        ######################\r\n",
        "\r\n",
        "        # DTT In the pix2pix original implementation, discriminator's gradients\r\n",
        "        #     are deactivated\r\n",
        "        #     ** self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\r\n",
        "\r\n",
        "        optimizer_g.zero_grad()\r\n",
        "\r\n",
        "        feature_fake = features(fake_b) # the pre-trained VGG   \r\n",
        "        feature_real = features(real_b) # the pre-trained VGG  \r\n",
        "\r\n",
        "        # First, G(A) should fake the discriminator\r\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\r\n",
        "        pred_fake = net_d.forward(fake_ab)\r\n",
        "        loss_g_gan = criterionGAN(pred_fake, True)\r\n",
        "\r\n",
        "        # Second, G(A) = B\r\n",
        "        loss_g_l1 = criterionL1(fake_b, real_b) * opt.lamb\r\n",
        "        loss_g_vgg = criterionvgg(feature_fake, feature_real) * (opt.lamb2) # VGG loss\r\n",
        "        loss_g = loss_g_gan + loss_g_l1 + loss_g_vgg # añadimos VGG loss\r\n",
        "        loss_g.backward()\r\n",
        "        optimizer_g.step()\r\n",
        "\r\n",
        "        # DTT Let's print just some iteration messages per epoch\r\n",
        "        #     Iterations go from 1 to ceiling(len(train_set) / batch_size)\r\n",
        "        if iteration % max((math.ceil(len(train_set) / opt.batch_size) // opt.iter_messages), 1) == 0:\r\n",
        "            print(\"===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\".format(\r\n",
        "                epoch, iteration, len(training_data_loader), loss_d.item(), loss_g.item()))\r\n",
        "        \r\n",
        "        # DTT Logging the same data for TensorBoard analysis\r\n",
        "        if opt.tb_active:\r\n",
        "            save_iteration_tensorboard(writer_train, epoch, iteration, loss_d, loss_g, loss_g_gan, loss_g_l1, loss_g_vgg,\r\n",
        "                               real_a, real_b, fake_b, batch)\r\n",
        "\r\n",
        "    # Only execute if a minimum epochs are expected\r\n",
        "    if (opt.niter + opt.niter_decay + 1) > opt.checkpoint_epochs:\r\n",
        "        update_learning_rate(net_g_scheduler, optimizer_g)\r\n",
        "        update_learning_rate(net_d_scheduler, optimizer_d)\r\n",
        "\r\n",
        "    # test\r\n",
        "    avg_psnr = 0\r\n",
        "    for batch in testing_data_loader:\r\n",
        "        input, target = batch[0].to(device), batch[1].to(device)\r\n",
        "\r\n",
        "        prediction = net_g(input)\r\n",
        "        mse = criterionMSE(prediction, target)\r\n",
        "        psnr = 10 * log10(1 / mse.item())\r\n",
        "        avg_psnr += psnr\r\n",
        "    \r\n",
        "    time_spent = time.time() - epoch_start_time\r\n",
        "    print(\"===> Avg. PSNR: {:.4f} dB. Time spent in epoch {} is {:.4f}\".format(avg_psnr / len(testing_data_loader), epoch, time_spent))\r\n",
        "\r\n",
        "    if opt.tb_active:\r\n",
        "        # DTT I log the same data for TensorBoard analysis\r\n",
        "        writer_train.add_scalar('Metrics/Avg. PSNR', avg_psnr / len(testing_data_loader), epoch)\r\n",
        "        writer_train.add_scalar('Metrics/D''s LR', optimizer_d.param_groups[0]['lr'], epoch)\r\n",
        "        writer_train.add_scalar('Metrics/G''s LR', optimizer_g.param_groups[0]['lr'], epoch)\r\n",
        "        writer_train.add_scalar('Time spent', time_spent, epoch)\r\n",
        "\r\n",
        "\r\n",
        "    #checkpoint\r\n",
        "    exit = save_checkpoint(epoch, net_g, net_d, optimizer_g, optimizer_d)\r\n",
        "    if exit:\r\n",
        "        print(\"Ending training as stop_after_checkpoint is set to True\")\r\n",
        "        break\r\n",
        "\r\n",
        "if opt.tb_active:\r\n",
        "    writer_train.close()\r\n",
        "\r\n",
        "train_time = time.gmtime(time.time() - start_time)\r\n",
        "train_time = time.strftime(\"%H:%M:%S\",train_time)\r\n",
        "\r\n",
        "print(\"\\nTraining ended. It took {}\".format(train_time))\r\n",
        "print(\"Arguments used: {}\".format(training_args))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vb18-emCQlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60c6311-116b-48eb-db7e-699bc673fb6b"
      },
      "source": [
        "!ls dataset/train/gt|wc # 135 files\r\n",
        "!ls dataset/test/gt | wc\r\n",
        "# !rm checkpoint/*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    135     135    1779\n",
            "     30      30     392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhw6VB9yECIn"
      },
      "source": [
        "## Launch TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDN-_iThD4CH"
      },
      "source": [
        "# Load extension\r\n",
        "%reload_ext tensorboard\r\n",
        "# Run TensorBoard on training directory: takes some seconds\r\n",
        "# It doesn't work with python variables: %tensorboard --logdir dir_log_train  <-- fails\r\n",
        "# %tensorboard --logdir \"dataset/train/log\"\r\n",
        "%tensorboard --logdir '/content/drive/MyDrive/Colab Notebooks/AIDL/Project/nosplit542/log/1200epochs'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}